---
title: 'Code4Sac: 311 Service Call Data Analysis'
output:
  html_document:
    toc: true
---

Author: Walter Yu  
Organization: Code for Sacramento  
  
# Introduction
     
Code for Sarcramento is develop a neighborhood portal application; as a result, this notebook evaluates the City of Sacramento 311 service call dataset for insights and trends which may be helpful in developing useful features.  
    
This notebook will focus on an initial analysis and modeling effort with additional prediction to follow as more information is available regarding the application.  
  
Dataset:
  
311 service call dataset from City of Sacramento; summary statistics for the full and partial datasets are listed below. The data is available in geospatial, tabular or API format; the tabular format is used for this analysis to identify important neighorhoods and trends which are relevant to developing the application.  

City of Sacramento 311 Service Call Dataset: https://data.cityofsacramento.org/datasets/08794a6695b3483f889e9bef122517e9_0  

*** Summary Statistics - Full Dataset (~1.2M Rows x 27 Columns) ***  

'data.frame':	1225340 obs. of  27 variables:  
 $ X                    : num  -121 -122 -121 -122 -122 ...
 $ Y                    : num  38.6 38.5 38.4 38.6 38.5 ...
 $ OBJECTID             : int  6136 6137 6138 6139 6140 6141 6142 6143 6144 6145 ...
 $ ReferenceNumber      : Factor w/ 1130759 levels "160430-000040",..: 2999 3002 3003 3004 3005 3007 3006 3010 3008 3009 ...
 $ CategoryHierarchy    : Factor w/ 437 levels ":",": Code Enforcement",..: 342 340 326 318 323 326 339 363 326 71 ...
 $ CategoryLevel1       : Factor w/ 18 levels "","Animal care",..: 14 14 14 14 14 14 14 14 14 3 ...
 $ CategoryLevel2       : Factor w/ 48 levels "","Animal Cruelty (597)",..: 28 26 1 15 18 1 26 39 1 17 ...
 $ CategoryLevel3       : Factor w/ 10 levels "","Aggressive",..: 1 1 1 1 1 1 1 4 1 1 ...
 $ CategoryLevel4       : logi  NA NA NA NA NA NA ...
 $ CategoryLevel5       : logi  NA NA NA NA NA NA ...
 $ CategoryName         : Factor w/ 286 levels "","911 Transfer",..: 91 137 109 91 91 109 91 142 109 183 ...
 $ CouncilDistrictNumber: int  3 7 8 1 4 5 3 5 8 3 ...
 $ SourceLevel1         : Factor w/ 5 levels "","Email","Mobile App",..: 4 4 4 4 4 5 4 5 4 4 ...
 $ Neighborhood         : Factor w/ 130 levels "","Airport","Alhambra Triangle",..: 102 85 115 67 56 44 102 73 115 31 ...
 $ DateCreated          : Factor w/ 1118207 levels "2016-04-30T16:05:44.000Z",..: 2988 2989 2990 2991 2992 2993 2994 2995 2996 2997 ...
 $ DateUpdated          : Factor w/ 927449 levels "2016-04-30T16:46:53.000Z",..: 238837 146545 284898 238837 146543 241794 232773 238864 284900 295 ...
 $ DateClosed           : Factor w/ 772674 levels "","2016-05-30T18:53:03.000Z",..: 218275 136981 261213 218275 136981 221113 212237 218302 261213 1 ...
 $ StatusType           : Factor w/ 14 levels "Cancellation Completed",..: 4 3 4 4 3 4 4 4 4 8 ...
 $ SystemId             : int  13089 13091 13092 13095 13098 13100 13103 13107 13110 13112 ...
 $ ServiceLevelName     : Factor w/ 34 levels "","1 Hour-All Holidays-24/7",..: 13 13 1 1 1 1 13 1 1 29 ...
 $ Latitude             : num  38.6 38.5 38.4 38.6 38.5 ...
 $ Longitude            : num  -121 -122 -121 -122 -122 ...
 $ XCoord               : num  6706636 6692383 6730272 6700552 6697903 ...
 $ YCoord               : num  1988045 1940692 1925199 1993886 1953896 ...
 $ CrossStreet          : Factor w/ 11685 levels "","&","10TH AVE",..: 11552 9908 10313 1402 2119 6943 3935 3120 11242 6799 ...
 $ GlobalID             : Factor w/ 1225340 levels "00001d4c-8b09-4d05-a87c-46fbd73bc23d",..: 347721 686993 785273 361851 1165337 1078483 497589 933654 757256 194850 ...
 $ ZIP                  : Factor w/ 66 levels "","00000","11111",..: 53 51 32 54 42 42 53 40 43 36 ...

*** Summary Statistics - Subset Dataset (5k Rows x 8 Columns) ***  

'data.frame':	5000 obs. of  8 variables:  
 $ CategoryHierarchy    : Factor w/ 437 levels ":",": Code Enforcement",..: 342 340 326 318 323 326 339 363 326 71 ...
 $ CategoryName         : Factor w/ 286 levels "","911 Transfer",..: 91 137 109 91 91 109 91 142 109 183 ...
 $ CouncilDistrictNumber: int  3 7 8 1 4 5 3 5 8 3 ...
 $ SourceLevel1         : Factor w/ 5 levels "","Email","Mobile App",..: 4 4 4 4 4 5 4 5 4 4 ...
 $ Neighborhood         : Factor w/ 130 levels "","Airport","Alhambra Triangle",..: 102 85 115 67 56 44 102 73 115 31 ...
 $ StatusType           : Factor w/ 14 levels "Cancellation Completed",..: 4 3 4 4 3 4 4 4 4 8 ...
 $ ServiceLevelName     : Factor w/ 34 levels "","1 Hour-All Holidays-24/7",..: 13 13 1 1 1 1 13 1 1 29 ...
 $ ZIP                  : Factor w/ 66 levels "","00000","11111",..: 53 51 32 54 42 42 53 40 43 36 ...
                                         CategoryHierarchy               CategoryName  CouncilDistrictNumber     SourceLevel1 
 Solid Waste : Household Junk                     :1487    Household Junk      :1487   Min.   :1.000                   :   0  
 Solid Waste : Appliance and eWaste               : 259    Garbage             : 930   1st Qu.:3.000         Email     : 181  
 Solid Waste : Kick truck : Garbage               : 223    Lawn and Garden     : 365   Median :5.000         Mobile App: 284  
 Solid Waste : Illegal Dumping : Street / Sidewalk: 181    Recycle             : 359   Mean   :4.601         Phone Call:3926  
 Solid Waste : Replace Can : Garbage              : 152    Appliance and eWaste: 259   3rd Qu.:6.000         Web App   : 609  
 Solid Waste : Exchange Can : Garbage             : 150    Street / Sidewalk   : 181   Max.   :8.000                          
 (Other)                                          :2548    (Other)             :1419   NA's   :27                             
                   Neighborhood                   StatusType                       ServiceLevelName      ZIP      
 Valley Hi / North Laguna: 388   Completed             :3907                               :3355    95822  : 533  
 East Sacramento         : 302   In Progress           : 519   36 Hours-No Holidays-M/F-24h: 602    95823  : 486  
 Meadowview              : 296   Closed                : 440   7 Days-No Holidays-M/F-24h  : 211    95820  : 423  
 Pocket                  : 246   Cancelled             : 132   2 Hours-All Holidays-24x7   : 158    95831  : 403  
 Downtown                : 222   Pending               :   2   24 Hours-No Holidays-M/F-24h: 157    95838  : 351  
 South Natomas           : 178   Cancellation Completed:   0   7 Days-All Holidays-24x7    :  75    95833  : 310  
 (Other)                 :3368   (Other)               :   0   (Other)                     : 442    (Other):2494  

Notes:  
1. All sources are cited accordingly.  
2. Source dataset is ~1.2M rows, so not included in this repository.  
3. As a result, please review attached HTML file for analysis/findings.  
  
** P0 **  
** P0 **  
** P0 **  

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Install any necessary packages:
# install.packages("FNN")

library(dplyr)
library(readr)
library(stringr)
library(ggplot2)
library(gridExtra)
library(leaps)
library(glmnet)
library(reshape2)
library(randomForest)
library(e1071)
library(FNN)

# LDA/QDA:
library(boot)
library(MASS)
library(class)
library(reshape2)

# Neural Net:
library(neuralnet)
library(tidyr)

```

** P1 **  
** P1 **  
** P1 **  

# Part 1A - Summary Statistics  

References:  
1. HES CSCI-E63c Midterm, HW9, HW10, HW11 code  
2. ISLR p.28; categorical variables  

Steps:  
1. Import data, evaluate dimensions  
2. Subset first 5k rows to reduce runtime  
3. Subset columns to only include relevant data  
4. Verify subset with summary statistics; dim, str and summary functions  

Observations:  
1. Import dataset; includes header so note so in read.table  
2. Use str() function to evaluate factors/variables  
3. Original dataset = ~1.2M rows x 27 columns  
4. Resulting dataset = 5k rows x 27 columns  
5. Test dataset may be extracted from full dataset  

```{r p1.1}

# import train_data:
train_data <- read.table(
  '../311_Calls_OSC_View.csv',
  sep=',',
  header=TRUE,
  quote='',
  strip.white=TRUE,
  na.strings='?'
)

```

```{r p1.2}

# verify dim and drop NA; train_data:
dim(train_data)

# reduce to smaller sample size to reduce runtime:
train_reduced = train_data[1:5000,]

# reduce to smaller sample size for relevant data:
train_col = train_reduced[, c(
  "CategoryHierarchy",
  "CategoryName",
  "CategoryLevel1",
  "CategoryLevel2",
  "CategoryLevel3",
  "CouncilDistrictNumber",
  "SourceLevel1",
  "Neighborhood",
  "StatusType",
  "ServiceLevelName",
  "ZIP"
)]

cat("\n")
cat("*** remove na null values ***")
cat("\n")

# verify dim and drop NA; train_data:
# which(is.na(train_col))
# dim(train_col)
# train_col = na.omit(train_col)
# dim(train_col)

cat("\n")
cat("*** output summary; train_data ***")
cat("\n")

# EDA; train_data:
dim(train_col)
str(train_col)
summary(train_col)

cat("\n")
cat("*** encode categorical variables as factors ***")
cat("\n")

# Encode categorial variables as factors:
# https://stats.idre.ucla.edu/r/modules/coding-for-categorical-variables-in-regression-models/

# Selected columns for relevant data:
# "CategoryHierarchy",
# "CategoryName",
# "CategoryLevel1",
# "CategoryLevel2",
# "CategoryLevel3",
# "CouncilDistrictNumber",
# "SourceLevel1",
# "Neighborhood",
# "StatusType",
# "ServiceLevelName",
# "ZIP"

train_col$cat_heir.f <- factor(train_col$CategoryHierarchy)
is.factor(train_col$cat_heir.f)

train_col$cat_name.f <- factor(train_col$CategoryName)
is.factor(train_col$cat_name.f)

train_col$cat_1.f <- factor(train_col$CategoryLevel1)
is.factor(train_col$cat_1.f)

train_col$cat_2.f <- factor(train_col$CategoryLevel2)
is.factor(train_col$cat_2.f)

train_col$cat_3.f <- factor(train_col$CategoryLevel3)
is.factor(train_col$cat_3.f)

train_col$district.f <- factor(train_col$CouncilDistrictNumber)
is.factor(train_col$district.f)

train_col$source.f <- factor(train_col$SourceLevel1)
is.factor(train_col$source.f)

train_col$neighborhood.f <- factor(train_col$Neighborhood)
is.factor(train_col$neighborhood.f)

train_col$status.f <- factor(train_col$StatusType)
is.factor(train_col$status.f)

train_col$service.f <- factor(train_col$ServiceLevelName)
is.factor(train_col$service.f)

train_col$zip.f <- factor(train_col$ZIP)
is.factor(train_col$zip.f)

# convert into numeric values:
# train_col_numeric = train_col
# train_col_numeric %>% mutate_if(is.factor, as.numeric)

# convert with lapply function:
# https://stackoverflow.com/questions/47922184/convert-categorical-variables-to-numeric-in-r

train_col_numeric = train_col
i <- sapply(train_col_numeric, is.factor)
train_col_numeric[i] <- lapply(train_col_numeric[i], as.numeric)

# verify columns as numeric:
# str(train_col_numeric)
# head(train_col_numeric, 5)

cat("\n")
cat("*** summary statistics for numeric values ***")
cat("\n")

summary(train_col_numeric)

# save code in case prediction/validation is necessary:
# import test_data:
# test_data <- read.table(
#   './final-data-test.csv',
#   sep=',',
#   header=TRUE,
#   quote='',
#   strip.white=TRUE,
#   na.strings='?'
# )

# verify dim and drop NA; test_data:
# dim(test_data)
# na.omit(test_data)
# dim(test_data)
# head(test_data, 5)

# summary statistics; test_data:
# str(test_data)
# summary(test_data)

# reduce dataset to minimize runtime:
# test_reduced = test_data[1:1500,]

```

References:  
1. HES CSCI-E63c Midterm, HW5 code  
2. ISLR p.67; p-value  

Steps:  
1. Create histogram to evaluate distribution  
2. Create stacked bar histograms  
3. Evaluate trends from plots; identify possible trends  
4. Create plots for significant factors bsed on p-values  
5. Document observations below  

Observations:  
1. Create histograms for significant factors based on p-values  
2. Create stacked bar charts for significant factors based on p-values  
3. Stacked bar chart shows interaction between neighborhood and service level  

```{r p1.2A}

# Create historgram for continous variables to evaluate distribution:

hist(
  train_col_numeric$district.f,
  main='Council District',
  col='purple',
  xlab='Continuous Variable - Council District',
  breaks=30
)

hist(
  train_col_numeric$service.f,
  main='311 Service Level',
  col='purple',
  xlab='Continuous Variable - 311 Service Level',
  breaks=10
)

# Create histogram, stacked bar chart; remaining variables
qplot_district = (qplot(
  train_col_numeric$district.f,
  binwidth = 1,
  fill=train_col_numeric$service.f,
  main='Outcome Variable - 311 Service Level')
  + scale_fill_manual(values=c('purple', 'red'))
  + theme(legend.position='top')
  + xlab('Predictor Variable - Council District Number')
)
qplot_district

```

References:  
1. HES CSCI-E63c Midterm, HW9, HW10, HW11 code  
2. ISLR p.67; data distribution  
3. ISLR p.76; data distribution    

Steps:  
1. Evaluate data distribution with contingency tables  
2. Evaluate distribution, then adjust for proportions  
3. Create contingency tables for select attributes   
4. Compare/evaluate results  
5. Document observations below   

Observations:  
1. Create contingency tables for significant factors based on p-values  
2. Contingency tables show distributions of variables and breaks them down by level  
3. In general, contingency tables are good for visualizing distributions by level  

```{r p1.3}

cat("\n")
cat("*** contingency table for neighborhood and source ***")
cat("\n")

# Create contingency table:
table(
  train_data$Neighborhood,
  train_data$SourceLevel1
)

cat("\n")
cat("*** contingency table for zipcode and source ***")
cat("\n")

# Create contingency table:
table(
  train_data$ZIP,
  train_data$SourceLevel1
)

```

# Part 1B - Logistic Regression  

References:  
1. HES CSCI-E63c Midterm, HW5, HW6; glm model  
2. ISLR p.67; p-value and significance  
3. ISLR p.286; logistic regression  
4. ISLR p.291; logistic regression usage  

Steps:  
1. Fit logistic regression model for predictor variables  
2. Evaluate/compare results of each variable  
3. Specifically, evaluate p-values of logistic regression output  
4. Evaluate results for association between predictor/outcome  

Observations:  
1. Most variables have low p-values, indicating significance as a predictor  
2. Use LR to fit categorical variables for additional analysis  

```{r p1.4}

cat("\n")
cat("*** logistic regression model fit ***")
cat("\n")

# logistic regression model fit:
glm_train = glm(
  Neighborhood~source.f,
  data=train_col,
  family=binomial
)
summary(glm_train)

# logistic regression model fit:
glm_train = glm(
  Neighborhood~service.f,
  data=train_col,
  family=binomial
)
summary(glm_train)

# logistic regression model fit:
glm_train = glm(
  Neighborhood~cat_1.f,
  data=train_col,
  family=binomial
)
summary(glm_train)

# logistic regression model fit:
glm_train = glm(
  Neighborhood~cat_2.f,
  data=train_col,
  family=binomial
)
summary(glm_train)

# logistic regression model fit:
glm_train = glm(
  Neighborhood~cat_3.f,
  data=train_col,
  family=binomial
)
summary(glm_train)

# logistic regression model fit:
glm_train = glm(
  ZIP~source.f,
  data=train_col,
  family=binomial
)
summary(glm_train)

# logistic regression model fit:
glm_train = glm(
  ZIP~service.f,
  data=train_col,
  family=binomial
)
summary(glm_train)

# logistic regression model fit:
glm_train = glm(
  ZIP~cat_1.f,
  data=train_col,
  family=binomial
)
summary(glm_train)

# logistic regression model fit:
glm_train = glm(
  ZIP~cat_2.f,
  data=train_col,
  family=binomial
)
summary(glm_train)

# logistic regression model fit:
glm_train = glm(
  ZIP~cat_3.f,
  data=train_data,
  family=binomial
)
summary(glm_train)

```

References:  
1. HES CSCI-E63c Midterm, HW6; PCA model  
2. ISLR p.230; PCA components  
3. ISLR p.233; PCA components  

Steps:  
1. Verify/prepare data for PCA plots; use dummy variables to setup factors  
2. Scale/fit data for PCA plots; plot for top contributors to variance  
3. Evaluate top contributors to variance  
4. Plot variance by first several PCA  

Observations:  
1. Use dummy variables to address muiple variable levels prior to PCA plot  
2. PCA calculations show top contributors of variance to the model  
3. PCA plot show interaction between neighborhood and service level  

END **  

```{r p1.4}

cat("\n")
cat("*** Plot variance for first several PCA ***")
cat("\n")

# train_scaled = train_data %>% mutate_if(is.numeric, scale)
# train_dummy = model.matrix(noyes ~., train_scaled)[,-1]

# Scale/dummy data prior to plot:
# train_numeric = as.numeric(train_col$zip.f)
# train_scaled = train_numeric %>% mutate_if(is.numeric, scale)
# train_dummy = model.matrix(noyes ~., train_scaled)[,-1]

train_scaled = train_col_numeric %>% mutate_if(is.numeric, scale)
train_dummy = model.matrix(Neighborhood ~., train_scaled)[,-1]

prcomp_plot = prcomp(train_dummy)
plot(
  prcomp_plot,
  xlab='dimensions',
  col='purple'
)

cat("\n")
cat("*** Evaluate largest loading for first PC ***")
cat("\n")

sort(abs(prcomp_plot$rotation[,1]), decreasing=TRUE)[1]

cat("\n")
cat("*** Evaluate largest loading for second PC ***")
cat("\n")

sort(abs(prcomp_plot$rotation[,2]), decreasing=TRUE)[1]

cat("\n")
cat("*** Evaluate largest loading for third PC ***")
cat("\n")

sort(abs(prcomp_plot$rotation[,3]), decreasing=TRUE)[1]

cat("\n")
cat("*** Evaluate largest loading for fourth PC ***")
cat("\n")

sort(abs(prcomp_plot$rotation[,4]), decreasing=TRUE)[1]

cat("\n")
cat("*** Evaluate largest loading for fifth PC ***")
cat("\n")

sort(abs(prcomp_plot$rotation[,5]), decreasing=TRUE)[1]

cat("\n")
cat("*** Biplot: PC1 and PC2 (Scaled) ***")
cat("\n")

biplot(
  prcomp_plot,
  pc.biplot=TRUE,
  xlabs=rep('*', dim(prcomp_plot$x)[1]),
  col=c('red','purple')
)

cat("\n")
cat("*** PCA Plot ***")
cat("\n")

plot(
  prcomp_plot$x[,1:2],
  main='PCA Plot: Neighborhood and Service Level',
  col=c(ifelse(train_col_numeric$Neighborhood== '>=10', 'red', 'purple')),
  cex=1.0,
  pch=ifelse(train_col_numeric$ServiceLevelName== '>=1', '*', '+')
)

```

References:  
1. HES CSCI-E63c Lecture 1, 3 notes; chi-squared test  
2. ISLR p.67; p-value and significance  

Steps:  
1. Calculate chi-squared test for categorical variables  
2. Evaluate/analyze results  
3. Document observations below  

Observations:  
1. Chi-squared test shows low p-values for categorial variables; t-test for continous variables  
2. As a result, this observations confirms previous conjecture that variables should be kept as factors since they are significant predictors  
3. Chi-squared and t-test are a good method for evaluating significance  

Observations:  
1. Create plots for significant factors based on p-values  
2. Chi-squared and t-tests confirmed that all variables are significant per logistic regression results; specifically, all varaibles have p-value < 0.05  
3. As a result, these variables are significant and should be used for analysis  

```{r p1.5}

cat("\n")
cat("*** t-test for continous variables ***")
cat("\n")

# Chi-Squared test for continuous variables:
t.test(
  as.numeric(train_col$neighborhood.f),
  train_data$service.f
)

cat("\n")
cat("*** chi-squared test for categorical variables ***")
cat("\n")

# Chi-Squared test for categorical variables:
chisq.test(
  table(
    train_col$neighborhood.f,
    train_col$cat_1.f
  )
)

```

# Part 2 - Logistic Regression

References:  
1. HW2 code; pairs plot  
2. HW9 code; pairs plot  
3. HW10 code; pairs plot  

Steps:  
1. Create pairs plot for scaled, numeric predictor variables  
2. Verify that columns are numeric and use scaled dataset  
3. Create plots; document observations below  
4. Use pairs plot to verify variables for additional analysis  

Observations:  
1. Scale values so that pairs plot can be correct associations  
2. Only numeric values selected for pairs plot  
3. Run pairs plot first before moving on with remaining analysis 

```{r p2.1}

# calculated here for analysis; comment out to reduce runtime
# create pairs plot of scaled numeric predictors:
# columns_is_numeric = unlist(lapply(train_reduced, is.numeric))
# numeric_cols = names(train_data)[columns_is_numeric]

# train_scaled_numeric = train_scaled %>% select_if(is.numeric)
# pairs(
#   train_scaled_numeric,
#   pch=25,
#   cex=0.8,
#   col=c(ifelse(train_scaled$noyes == 'yes', 'purple', 'red')),
#   main='Pairs Plot - No/Yes (yes = purple; no = red)'
# )

```

References:  
1. Midterm-P2 and HW5; best subset variable selection  
2. ISLR p.244; best subset selection  
3. ISLR p.247; best subset selection  
4. Lecture 9, 12, 13 notes; best subset  

Steps:  
1. Use variable selection to identify best predictors  
2. Use forward, backward and seq replacement methods  
3. Create metrics plot and individual plot for each method  
4. Evaluate/compare plot results  
5. Document observations below  

Observations:  
1. Each method returns the same variables and order of selection  
3. Results confirm previous findings/analysis/conjecture  
4. Significant variables will be useful for fitting other models  
5. Best subset plot visualizes results for evaluation  

END **  

```{r p2.2}

cat("\n")
cat("*** helper function for best subset selection ***")
cat("\n")

# helper function for best subset selection:
summaryMetrics <- NULL
whichAll <- list()
my_methods = c('backward', 'forward', 'seqrep')

for ( myMthd in my_methods ) {
  method_metrics = NULL
  rsRes <- regsubsets(
    noyes~.,
    # train_scaled,
    train_scaled_reduced,
    method=myMthd,
    nvmax=ncol(train_scaled_reduced)-1
  )
  summRes <- summary(rsRes)
  whichAll[[myMthd]] <- summRes$which

  for ( metricName in c('rsq','rss','adjr2','cp','bic') ) {
    summaryMetrics <- rbind(summaryMetrics,
      data.frame(method=myMthd,metric=metricName,
      nvars=1:length(summRes[[metricName]]),
      value=summRes[[metricName]]))
    method_metrics = rbind(method_metrics,
      data.frame(method=myMthd,metric=metricName,
      nvars=1:length(summRes[[metricName]]),
      value=summRes[[metricName]]))
  }
}

cat("\n")
cat("*** plot subset ***")
cat("\n")

# plot best subset:
ggplot(
    summaryMetrics,
    aes(x=nvars,y=value,shape=method,colour=method)
) + geom_path() + geom_point() + facet_wrap(~metric,scales='free') + theme(legend.position='top')

# plot best subset:
old.par <- par(mfrow=c(1,1),ps=9,mar=c(5,7,2,1))
for ( myMthd in names(whichAll) ) {
  image(1:nrow(whichAll[[myMthd]]),
        1:ncol(whichAll[[myMthd]]),
        whichAll[[myMthd]],xlab="N(vars)",ylab="",
        xaxt="n",yaxt="n",breaks=c(-0.5,0.5,1.5),
        col=c("white","purple"),main=myMthd)
  axis(1,1:nrow(whichAll[[myMthd]]),rownames(whichAll[[myMthd]]))
  axis(2,1:ncol(whichAll[[myMthd]]),colnames(whichAll[[myMthd]]),las=2)
}

```

** START  

References:  
1. ISLR p.251; lasso/ridge regression -XX  
2. HW5, midterm code; lasso regression -XX  
3. HW5, midterm code; ridge regression -XX  
4. Lecture 5 notes; lasso/ridge -XX  
5. -XX  

Steps:  
1. Fit logistic regression model and calculate summary statistics -XX  
2. Fit and plot lasso model -XX  
3. Determine best lambda values -XX  
4. Determine lambda value; 1 SD away -XX  
5. Document observations below -XX  

Observations:  
1. Make calculations using scaled data -XX  
2. Additional calculations made to evaluate error rates -XX  
3. Error rate also calculated for 1SD away -XX  
4. -XX  
5. -XX  

END **  

```{r final-p2.3}

cat("\n")
cat("*** fit logistic regression model and calculate summary statistics ***")
cat("\n")

# fit logistic regression model and calculate summary statistics (scaled):
train_glm_scaled = glm(
  noyes~.,
  # data=train_scaled,
  data=train_scaled_reduced,
  family=binomial
)
summary(train_glm_scaled)

cat("\n")
cat("*** fit and plot lasso model ***")
cat("\n")

# fit and plot lasso model:
# x = model.matrix(noyes~., train_scaled)[,-1]
x = model.matrix(noyes~., train_scaled_reduced)[,-1]

# y = train_scaled[,'noyes']
y = train_scaled_reduced[,'noyes']

lasso_fit_lr = glmnet(
  x, y,
  alpha=1,
  family='binomial'
)
plot(lasso_fit_lr)

cat("\n")
cat("*** determine best lambda values ***")
cat("\n")

# determine best lambda values:
cv_lasso_fit_lr = cv.glmnet(
  x, y,
  family='binomial',
  type.measure='class'
)
plot(cv_lasso_fit_lr)

cat("\n")
cat("*** determine lambda value; 1 SD away ***")
cat("\n")

# determine best lambda values; 1 SD away:
lowest_error_plus_one_sd_lambda = cv_lasso_fit_lr$lambda.1se

# evaluate which dashed lines is best:
print(
  log(lowest_error_plus_one_sd_lambda)
)

cat("\n")
cat("*** lasso model prediction ***")
cat("\n")

# determine best lambda values; 1 SD away:
predict(
  lasso_fit_lr,
  type='coefficients',
  s=lowest_error_plus_one_sd_lambda
)

```

** START  

References:  
1. HW9-P1 code; helper function to assess prediction quality -XX  
2. HW9-P4 code; helper function for cross validation -XX  
3. ISLR p.236; cross validation -XX  
4. Lecture 9, 10, 11 notes; cross validation -XX  
5. Lecture 9 notes; prediction quality function -XX  

Steps:  
1. Create helper function to assess prediction quality -XX  
2. Create helper function for cross validation -XX  
3. Print and evaluate assessment results -XX  
4. Plot and evaluate cross validation results -XX  
5. Document results below -XX  

Observations:  
1. Accuracy/specificity were ~84% per measures plot -XX  
2. In general, LR model fit was quite good -XX  
3. Fit based on variables selected in previous sections -XX  
4. Measures will be used to compare against other model fits -XX  
5. Measures plot are a good method of evaluating metrics -XX  
6. Model fit with ~84% accuracy, ~50% sensitivity and ~95% specificity -XX  

END **  

```{r final-p2.4}

cat("\n")
cat("*** helper function to predict model quality ***")
cat("\n")

# helper function to predict model quality:
assess_prediction = function(truth, predicted, print_results=FALSE) {
  # check for missing values (we are going to
  # compute metrics on non-missing values only)
  predicted = predicted[ ! is.na(truth) ]
  truth = truth[ ! is.na(truth) ]
  truth = truth[ ! is.na(predicted) ]
  predicted = predicted[ ! is.na(predicted) ]

  NotNa=length(truth)
  # how predictions align against known
  # training/testing outcomes:
  # TP/FP= true/false positives,
  # TN/FN=true/false negatives
  TP = sum(truth==1 & predicted==1)
  TN = sum(truth==0 & predicted==0)
  FP = sum(truth==0 & predicted==1)
  FN = sum(truth==1 & predicted==0)
  P = TP+FN # total number of positives in the truth data
  N = FP+TN # total number of negatives

  accuracy_pct = signif(sum(truth==predicted)*100/length(truth),3)
  error_rate_pct = 100-accuracy_pct
  sensitivity_pct = signif(100*TP/P,3)
  specificity_pct = signif(100*TN/N,3)
  precision_pct = signif(100*TP/(TP+FP),3)
  false_discovery_pct = signif(100*FP/(TP+FP),3)
  false_positive_rate_pct = signif(100*FP/N,3)

  if (print_results){
    cat("Total cases that are not NA: ",
    NotNa,"\n",sep="")

    # overall accuracy of the test: how many cases
    # (both positive and negative) we got right:
    cat("Correct predictions (accuracy): ",
      sum(truth==predicted),
      "(",accuracy_pct,"%)\n",sep="")
    cat("TPR (sensitivity)=TP/P: ", sensitivity_pct, "%\n", sep="")
    cat("TNR (specificity)=TN/N: ", specificity_pct, "%\n", sep="")
    cat("PPV (precision)=TP/(TP+FP): ", precision_pct, "%\n", sep="")
    cat("FDR (false discovery)=1-PPV: ", false_discovery_pct, "%\n", sep="")
    cat("FPR =FP/N=1-TNR: ", false_positive_rate_pct, "%\n", sep="")

    print('TP  TN  FP  FN')
    print(paste0(TP, ' ', TN, ' ', FP, ' ', FN))
  }

  return (list(NotNA=length(truth),
              accuracy_pct=accuracy_pct,
              error_rate_pct=error_rate_pct,
              sensitivity_pct=sensitivity_pct,
              specificity_pct=specificity_pct,
              precision_pct=precision_pct,
              false_discovery_pct=false_discovery_pct,
              false_positive_rate_pct=false_positive_rate_pct))
}

cat("\n")
cat("*** helper function for cross validation ***")
cat("\n")

# helper function for cross validation:
# xvalNoYes= function(data, nTries=20, kXval=5) {

# decrease parameters to reduce runtime:
xvalNoYes= function(data, nTries=5, kXval=3) {
  retRes = NULL
  # set.seed(63)
  for ( iTry in 1:nTries ) {
    # assign each observation to one of the kXval folds
    xvalFolds = sample(rep(1:kXval, length.out=nrow(data)))

      measures <- NULL
      for ( kFold in 1:kXval ) {
        train = data[xvalFolds!=kFold,]
        test = data[xvalFolds==kFold,]

        # *** significant factors based on p-values (1.6, logistic regression) ***
        # categorial = cle, kbc, gb, xxp, rao, at, fq
        # numerical = mt, zq, zf, ihj

        # use the variables selected by both glm and lasso
        glm_fit = glm(
          noyes~cle+kbc+xxp+rao+at+fq,
          data=train,
          family=binomial
        )

        # predict on the held-out fold
        glm_predict = predict(glm_fit, newdata=test, type='response')

        test_predict = ifelse(glm_predict > 0.5, 1, 0)
        test_noyes_num = ifelse(test$noyes == 'yes', 1, 0)
        test_assessment_measures = assess_prediction(
          test_noyes_num,
          test_predict,
          print_results=FALSE
        )

        # accumulate test measurements over all cross-validation folds:
        measures = rbind(measures, cbind(test_assessment_measures$accuracy_pct,
          test_assessment_measures$sensitivity_pct,
          test_assessment_measures$specificity_pct))
      }

      print(measures)
      measure_means = colMeans(measures)

      print(measure_means)
      retRes = rbind(retRes, data.frame(sim=iTry,
        accuracy_pct=measure_means[1],
        sensitivity_pct=measure_means[2],
        specificity_pct=measure_means[3])
      )

  }
  retRes
}

# decrease parameters to reduce runtime:
# number_of_folds = 5
number_of_folds = 5

# decrease parameters to reduce runtime:
# number_of_tries = 15
number_of_tries = 3

```

```{r final-p2.4A, results="hide"}

# reduce to smaller sample size to reduce runtime:
# train_scaled[1:5000,]
df_output = xvalNoYes(
  train_scaled,
  kXval=number_of_folds,
  nTries=number_of_tries
)

```

```{r fig.width=9, fig.height=6, echo=FALSE}

cat("\n")
cat("*** cross validation plot ***")
cat("\n")

# cross validation plot:
df_output_melted = melt(df_output, id.vars=c('sim'))

p = ggplot(
  df_output_melted,
  aes(x=variable, y=value, colour=variable)
) + geom_boxplot()

title = sprintf(
  'Performance Measures Plot for LR; %d-fold Cross Validation',
  number_of_folds
)

p + ggtitle(title) + xlab("Measure Type") + ylab("Measure Value %")

```

## Extra points problem: interaction terms (5 extra points)

Assess the impact/significance of pairwise interaction terms for all pairwise combinations of covariates used in the model and report the top ten that most significantly improve model fit.

** P2A **  
** P2A **  
** P2A **  

** START  

References:  
1. HW2 code; pairs plot -XX  
2. HW9 code; pairs plot -XX  
3. HW10 code; pairs plot -XX  
4. Lecture 2, 9, 10 notes; pairs plot -XX  
5. -XX  

Steps:  
1. Create pairs plot for scaled, numeric predictor variables -XX  
2. Verify that columns are numeric and use scaled dataset -XX  
3. Create plots; document observations below -XX  
4. Completed previously but repeated here -XX  
5. Plot completed at start of P2 for use in analysis -XX  

Observations:  
1. Scale values so that pairs plot can be correct associations -XX  
2. Only numeric values selected for pairs plot -XX  
3. Run pairs plot first before moving on with remaining analysis -XX  
4. Completed previously but repeated here -XX  
5. Plot completed at start of P2 for use in analysis -XX  

END **  

*** significant factors based on p-values (1.6, logistic regression) ***
categorial = cle, kbc, gb, xxp, rao, at, fq
numerical = mt, zq, zf, ihj

```{r final-p2A.1}

cat("\n")
cat("*** create pairs plots ***")
cat("\n")

# *** significant factors based on p-values (1.6, logistic regression) ***
# categorial = cle, kbc, gb, xxp, rao, at, fq
# numerical = mt, zq, zf, ihj

# create pairs plot of scaled numeric predictors:
# columns_is_numeric = unlist(lapply(train_data, is.numeric))
columns_is_numeric = unlist(
  lapply(train_reduced, is.numeric)
)

# numeric_cols = names(train_data)[columns_is_numeric]
numeric_cols = names(train_reduced)[columns_is_numeric]

# train_scaled_numeric = train_scaled %>% select_if(is.numeric)
train_scaled_numeric = train_scaled_reduced %>% select_if(is.numeric)

pairs(
  train_scaled_numeric,
  pch=25,
  cex=0.8,

  # col=c(ifelse(train_scaled$noyes == 'yes', 'purple', 'red')),
  col=c(ifelse(
    train_scaled_reduced$noyes == 'yes',
    'purple', 'red'
  )),

  main='Pairs Plot - No/Yes (yes = purple; no = red)'
)

```

# Problem 3: linear discriminant analysis (15 points)

Fit linear discriminant analysis model of the outcome `noyes` as a function of the rest of covariates in the dataset.  Feel free to decide whether you want to use all of them or a subset of those.  Test resulting model performance on multiple splits of the data into training and test subsets, summarize it in terms of accuracy/error/sensitivity/specificity and compare them to those obtained for logistic regression.

** P3 **  
** P3 **  
** P3 **  

** START  

References - HW9-P2.1:  
1. Slide 18, lecture 9 notes; LDA/QDA -XX  
2. Slide 27, lecture 9 notes; contingency table -XX  
3. Slide 27, lecture 9 notes; assess prediction quality -XX  
4. ISLR p.139; LDA -XX  
5. ISLR p.149; QDA -XX  

Steps:  
1. Fit LDA/QDA classifiers -XX  
2. Calculate LDA/QDA summary statistics -XX  
3. Make predictions with LDA/QDA classifiers -XX  
4. Create contingency tables to evaluate LDA/QDA classifiers -XX  
5. Document observations below -XX  

Observations:  
1. Contingency table helped to visualize results -XX  
2. Contingency table showed reasonably good model fit -XX  
3. Prediction quality function used to evaluate fit -XX  
4. Model fit with ~85% accruacy -XX  
5. However, other metrics did not calculate due to problems with variable/data type; ran out of time and did not get a chance to troubleshoot -XX  

END **  

```{r final-p3.1}

cat("\n")
cat("*** fit LDA/QDA classifiers ***")
cat("\n")

# Slide 18, lecture 9 notes; LDA/QDA:
train_lda = lda(
  noyes~.,
  data=train_data
)

cat("\n")
cat("*** LDA/QDA summary statistics ***")
cat("\n")

cat("\n")
summary(train_data)

cat("\n")
cat("*** create contingency table ***")
cat("\n")

# Slide 27, lecture 9 notes; contingency table:
train_predict_lda = predict(train_lda)$class
train_lda_contingency = table(
  train_predict_lda,
  train_data$noyes
)

cat("\n")
train_lda_contingency

cat("\n")
cat("*** assess prediction quality ***")
cat("\n")

# Slide 27, lecture 9 notes; contingency table:
train_lda_results = assess_prediction(
  train_data$noyes,
  train_predict_lda,
  print_results=TRUE
)

```

** START  

References - P4.2A code:  
1. HW9-P1 code; helper function to assess prediction quality -XX  
2. HW9-P4 code; helper function for cross validation -XX  
3. ISLR p.236; cross validation -XX  
4. ISLR p.139; LDA -XX  
5. ISLR p.149; QDA -XX  

Steps:  
1. Create helper function to assess prediction quality -XX  
2. Create helper function for cross validation -XX  
3. Print and evaluate assessment results -XX  
4. Plot and evaluate cross validation results -XX  
5. Document results below -XX  

Observations:  
1. Setup prediction quality and cross validation functions for model fit -XX  
2. Cross validation had error when performingmodel fit -XX  
3. However, ran out of time and did not get a chance to troubleshoot -XX  
4. Code is complete and ran for several other models -XX  
5. My guess would be to troubleshoot by limiting number of variables -XX  

END **  

```{r final-p3.2}

cat("\n")
cat("*** helper function to predict model quality ***")
cat("\n")

# helper function to predict model quality:
assess_prediction = function(truth, predicted, print_results=FALSE) {
  # check for missing values (we are going to
  # compute metrics on non-missing values only)
  predicted = predicted[ ! is.na(truth) ]
  truth = truth[ ! is.na(truth) ]
  truth = truth[ ! is.na(predicted) ]
  predicted = predicted[ ! is.na(predicted) ]

  NotNa=length(truth)
  # how predictions align against known
  # training/testing outcomes:
  # TP/FP= true/false positives,
  # TN/FN=true/false negatives
  TP = sum(truth==1 & predicted==1)
  TN = sum(truth==0 & predicted==0)
  FP = sum(truth==0 & predicted==1)
  FN = sum(truth==1 & predicted==0)
  P = TP+FN # total number of positives in the truth data
  N = FP+TN # total number of negatives

  accuracy_pct = signif(sum(truth==predicted)*100/length(truth),3)
  error_rate_pct = 100-accuracy_pct
  sensitivity_pct = signif(100*TP/P,3)
  specificity_pct = signif(100*TN/N,3)
  precision_pct = signif(100*TP/(TP+FP),3)
  false_discovery_pct = signif(100*FP/(TP+FP),3)
  false_positive_rate_pct = signif(100*FP/N,3)

  if (print_results){
    cat("Total cases that are not NA: ",
    NotNa,"\n",sep="")

    # overall accuracy of the test: how many cases
    # (both positive and negative) we got right:
    cat("Correct predictions (accuracy): ",
      sum(truth==predicted),
      "(",accuracy_pct,"%)\n",sep="")
    cat("TPR (sensitivity)=TP/P: ", sensitivity_pct, "%\n", sep="")
    cat("TNR (specificity)=TN/N: ", specificity_pct, "%\n", sep="")
    cat("PPV (precision)=TP/(TP+FP): ", precision_pct, "%\n", sep="")
    cat("FDR (false discovery)=1-PPV: ", false_discovery_pct, "%\n", sep="")
    cat("FPR =FP/N=1-TNR: ", false_positive_rate_pct, "%\n", sep="")

    print('TP  TN  FP  FN')
    print(paste0(TP, ' ', TN, ' ', FP, ' ', FN))
  }

  return (list(NotNA=length(truth),
              accuracy_pct=accuracy_pct,
              error_rate_pct=error_rate_pct,
              sensitivity_pct=sensitivity_pct,
              specificity_pct=specificity_pct,
              precision_pct=precision_pct,
              false_discovery_pct=false_discovery_pct,
              false_positive_rate_pct=false_positive_rate_pct))
}

cat("\n")
cat("*** helper function for cross validation ***")
cat("\n")

# helper function for cross validation:
# xvalNoYesLDA = function(data, nTries=20, kXval=5) {

# decrease parameters to reduce runtime:
xvalNoYesLDA = function(data, nTries=5, kXval=3) {
  retRes = NULL
  # set.seed(63)
  for ( iTry in 1:nTries ) {
    # assign each observation to one of the kXval folds
    xvalFolds = sample(rep(1:kXval, length.out=nrow(data)))

      measures <- NULL
      for ( kFold in 1:kXval ) {
        train = data[xvalFolds!=kFold,]
        test = data[xvalFolds==kFold,]

        # *** significant factors based on p-values (1.6, logistic regression) ***
        # categorial = cle, kbc, gb, xxp, rao, at, fq
        # numerical = mt, zq, zf, ihj

        # train_lda = lda(noyes~., data=train_reduced)
        train_lda = lda(noyes~., data=train_reduced)

        # predict on the held-out fold
        lda_predict = predict(train_lda, newdata=test, type='response')

        # *** ERROR ***
        # Error in ifelse(lda_predict > 0.5, 1, 0) : (list) object cannot be coerced to type 'double'
        # *** ERROR ***

        test_predict = ifelse(lda_predict > 0.5, 1, 0)
        test_noyes_num = ifelse(test$noyes == 'yes', 1, 0)
        test_assessment_measures = assess_prediction(
          test_noyes_num,
          test_predict,
          print_results=FALSE
        )

        # accumulate test measurements over all cross-validation folds:
        measures = rbind(measures, cbind(test_assessment_measures$accuracy_pct,
          test_assessment_measures$sensitivity_pct,
          test_assessment_measures$specificity_pct))
      }

      print(measures)
      measure_means = colMeans(measures)

      print(measure_means)
      retRes = rbind(retRes, data.frame(sim=iTry,
        accuracy_pct=measure_means[1],
        sensitivity_pct=measure_means[2],
        specificity_pct=measure_means[3])
      )

  }
  retRes
}

# decrease parameters to reduce runtime:
# number_of_folds = 5
number_of_folds = 5

# decrease parameters to reduce runtime:
# number_of_tries = 15
number_of_tries = 3

```

```{r final-p3.2A, results="hide"}
# *** DEBUG LATER ***
# reduce to smaller sample size to reduce runtime:
# train_scaled[1:5000,]
```

df_output = xvalNoYesLDA(
  train_reduced,
  kXval=number_of_folds,
  nTries=number_of_tries
)

```{r fig.width=9, fig.height=6, echo=FALSE}
# *** DEBUG LATER ***

cat("\n")
cat("*** cross validation plot ****")
cat("\n")

# cross validation plot:
```

df_output_melted = melt(
  df_output,
  id.vars=c('sim')
)

p = ggplot(
  df_output_melted,
  aes(x=variable, y=value, colour=variable)
) + geom_boxplot()

title = sprintf(
  'Performance Measures Plot for LR; %d-fold Cross Validation',
  number_of_folds
)

p + ggtitle(title) + xlab("Measure Type") + ylab("Measure Value %")

## Extra points problem: quadratic discriminant analysis (5 extra points)

In our experience attempting to fit quadratic discriminant analysis model of the categorical outcome `noyes` on this data results in a rank deficiency related error. Determine on how to correct this error and report resulting model training and test error/accuracy/etc. and how it compares to LDA and logistic regression above.

** P3A **  
** P3A **  
** P3A **  

** START  

References - HW9-P2.2:  
1. Slide 18, lecture 9 notes; LDA/QDA -XX  
2. Slide 27, lecture 9 notes; contingency table -XX  
3. Slide 27, lecture 9 notes; assess prediction quality -XX  
4. ISLR p.139; LDA -XX  
5. ISLR p.149; QDA -XX  

Steps:  
1. Fit LDA/QDA classifiers -XX  
2. Calculate LDA/QDA summary statistics -XX  
3. Make predictions with LDA/QDA classifiers -XX  
4. Create contingency tables to evaluate LDA/QDA classifiers -XX  
5. Document observations below -XX  

Observations:  
1. Contingency table helped to visualize results -XX  
2. Contingency table showed reasonably good model fit -XX  
3. Prediction quality function used to evaluate fit -XX  
4. Model fit with ~78% accruacy -XX  
5. However, other metrics did not calculate due to problems with variable/data type; ran out of time and did not get a chance to troubleshoot -XX  

END **  

```{r final-p3A.1}

# *** significant factors based on p-values (1.6, logistic regression) ***
# categorial = cle, kbc, gb, xxp, rao, at, fq
# numerical = mt, zq, zf, ihj

cat("\n")
cat("*** fit LDA/QDA classifiers ***")
cat("\n")

# Slide 18, lecture 9 notes; LDA/QDA:
# train_qda = qda(noyes~., data=train_data)
train_qda = qda(
  noyes~ptj+mt+zq+zf+ihj,
  data=train_data
)

cat("\n")
cat("*** LDA/QDA summary statistics ***")
cat("\n")

cat("\n")
summary(train_qda)

cat("\n")
cat("*** create contingency table ***")
cat("\n")

# Slide 27, lecture 9 notes; contingency table:
train_predict_qda = predict(train_qda)$class
train_qda_contingency = table(
  train_predict_qda,
  train_data$noyes
)

cat("\n")
cat("*** create contingency table ***")
cat("\n")

cat("\n")
train_qda_contingency

cat("\n")
cat("*** assess prediction quality ***")
cat("\n")

# Slide 27, lecture 9 notes; assess prediction quality:
train_qda_results = assess_prediction(
  train_data$noyes,
  train_predict_qda,
  print_results=TRUE
)

```

** START  

References - P4.2A code:  
1. HW9-P1 code; helper function to assess prediction quality -XX  
2. HW9-P4 code; helper function for cross validation -XX  
3. ISLR p.236; cross validation -XX  
4. ISLR p.139; LDA -XX  
5. ISLR p.149; QDA -XX  

Steps:  
1. Create helper function to assess prediction quality -XX  
2. Create helper function for cross validation -XX  
3. Print and evaluate assessment results -XX  
4. Plot and evaluate cross validation results -XX  
5. Document results below -XX  

Observations:  
1. Setup prediction quality and cross validation functions for model fit -XX  
2. Cross validation had error when performingmodel fit -XX  
3. However, ran out of time and did not get a chance to troubleshoot -XX  
4. Code is complete and ran for several other models -XX  
5. My guess would be to troubleshoot by limiting number of variables -XX  

END **  

```{r final-p3A.2}

cat("\n")
cat("*** helper function to predict model quality ***")
cat("\n")

# helper function to predict model quality:
assess_prediction = function(truth, predicted, print_results=FALSE) {
  # check for missing values (we are going to
  # compute metrics on non-missing values only)
  predicted = predicted[ ! is.na(truth) ]
  truth = truth[ ! is.na(truth) ]
  truth = truth[ ! is.na(predicted) ]
  predicted = predicted[ ! is.na(predicted) ]

  NotNa=length(truth)
  # how predictions align against known
  # training/testing outcomes:
  # TP/FP= true/false positives,
  # TN/FN=true/false negatives
  TP = sum(truth==1 & predicted==1)
  TN = sum(truth==0 & predicted==0)
  FP = sum(truth==0 & predicted==1)
  FN = sum(truth==1 & predicted==0)
  P = TP+FN # total number of positives in the truth data
  N = FP+TN # total number of negatives

  accuracy_pct = signif(sum(truth==predicted)*100/length(truth),3)
  error_rate_pct = 100-accuracy_pct
  sensitivity_pct = signif(100*TP/P,3)
  specificity_pct = signif(100*TN/N,3)
  precision_pct = signif(100*TP/(TP+FP),3)
  false_discovery_pct = signif(100*FP/(TP+FP),3)
  false_positive_rate_pct = signif(100*FP/N,3)

  if (print_results){
    cat("Total cases that are not NA: ",
    NotNa,"\n",sep="")

    # overall accuracy of the test: how many cases
    # (both positive and negative) we got right:
    cat("Correct predictions (accuracy): ",
      sum(truth==predicted),
      "(",accuracy_pct,"%)\n",sep="")
    cat("TPR (sensitivity)=TP/P: ", sensitivity_pct, "%\n", sep="")
    cat("TNR (specificity)=TN/N: ", specificity_pct, "%\n", sep="")
    cat("PPV (precision)=TP/(TP+FP): ", precision_pct, "%\n", sep="")
    cat("FDR (false discovery)=1-PPV: ", false_discovery_pct, "%\n", sep="")
    cat("FPR =FP/N=1-TNR: ", false_positive_rate_pct, "%\n", sep="")

    print('TP  TN  FP  FN')
    print(paste0(TP, ' ', TN, ' ', FP, ' ', FN))
  }

  return (list(NotNA=length(truth),
              accuracy_pct=accuracy_pct,
              error_rate_pct=error_rate_pct,
              sensitivity_pct=sensitivity_pct,
              specificity_pct=specificity_pct,
              precision_pct=precision_pct,
              false_discovery_pct=false_discovery_pct,
              false_positive_rate_pct=false_positive_rate_pct))
}

cat("\n")
cat("*** helper function for cross validation ***")
cat("\n")

# helper function for cross validation:
# xvalNoYesLDA = function(data, nTries=20, kXval=5) {

# decrease parameters to reduce runtime:
xvalNoYesQDA = function(data, nTries=5, kXval=3) {
  retRes = NULL
  # set.seed(63)
  for ( iTry in 1:nTries ) {
    # assign each observation to one of the kXval folds
    xvalFolds = sample(rep(1:kXval, length.out=nrow(data)))

      measures <- NULL
      for ( kFold in 1:kXval ) {
        train = data[xvalFolds!=kFold,]
        test = data[xvalFolds==kFold,]

        # *** significant factors based on p-values (1.6, logistic regression) ***
        # categorial = cle, kbc, gb, xxp, rao, at, fq
        # numerical = mt, zq, zf, ihj

        train_qda = qda(noyes~., data=train_reduced)

        # predict on the held-out fold
        qda_predict = predict(train_qda, newdata=test, type='response')

        # *** ERROR ***
        # Error in ifelse(lda_predict > 0.5, 1, 0) : (list) object cannot be coerced to type 'double'
        # *** ERROR ***

        test_predict = ifelse(qda_predict > 0.5, 1, 0)
        test_noyes_num = ifelse(test$noyes == 'yes', 1, 0)
        test_assessment_measures = assess_prediction(
          test_noyes_num,
          test_predict,
          print_results=FALSE
        )

        # accumulate test measurements over all cross-validation folds:
        measures = rbind(measures, cbind(test_assessment_measures$accuracy_pct,
          test_assessment_measures$sensitivity_pct,
          test_assessment_measures$specificity_pct))
      }

      print(measures)
      measure_means = colMeans(measures)

      print(measure_means)
      retRes = rbind(retRes, data.frame(sim=iTry,
        accuracy_pct=measure_means[1],
        sensitivity_pct=measure_means[2],
        specificity_pct=measure_means[3])
      )

  }
  retRes
}

# decrease parameters to reduce runtime:
# number_of_folds = 5
number_of_folds = 5

# decrease parameters to reduce runtime:
# number_of_tries = 15
number_of_tries = 3

```

```{r final-p3A.2A, results="hide"}
# *** DEBUG LATER ***
# reduce to smaller sample size to reduce runtime:
# train_scaled[1:5000,]
```

df_output = xvalNoYesQDA(
  train_reduced,
  kXval=number_of_folds,
  nTries=number_of_tries
)

```{r fig.width=9, fig.height=6, echo=FALSE}
# *** DEBUG LATER ***

cat("\n")
cat("*** cross validation plot ****")
cat("\n")

# cross validation plot:
```

df_output_melted = melt(
  df_output,
  id.vars=c('sim')
)

p = ggplot(
  df_output_melted,
  aes(x=variable, y=value, colour=variable)
) + geom_boxplot()

title = sprintf(
  'Performance Measures Plot for LR; %d-fold Cross Validation',
  number_of_folds
)

p + ggtitle(title) + xlab("Measure Type") + ylab("Measure Value %")

# Problem 4: random forest (15 points)

Develop random forest model of outcome `noyes`. Present variable importance plots and comment on relative importance of different attributes in the model.  Did attributes showing up as more important in random forest model also appear as significantly associated with the outcome by logistic regression?  Test model performance on multiple splits of data into training and test subsets, compare test and out-of-bag error estimates, summarize model performance in terms of accuracy/error/sensitivity/specificity and compare to the performance of logistic regression and LDA models above.

** P4 **  
** P4 **  
** P4 **  

** START  

References:  
1. ISLR p.303; tree-based methods -XX  
2. ISLR p.316, 320; RF model -XX  
3. HW11, HW12 code; RF model -XX  
4. Lecture 9, 10, 11 notes; RF model -XX  
5. -XX  

Steps:  
1. Develop/fit RF model -XX  
2. Calculate error rate -XX  
3. Create contingency table -XX  
4. Create variable importance plot -XX  
5. Document observations below -XX  

Observations:  
1. Contingency table helped to visualize results -XX  
2. OOB error rate was high; ~12% -XX  
3. Ran out of time to investigate further -XX  
4. However, code below is shown for initial fit -XX  
5. Continuing to remaining subparts -XX  

END **  

```{r final-p4.1}

cat("\n")
cat("*** fit RF model ***")
cat("\n")

# fit RF model:
set.seed(2)
train_rf_scaled = randomForest(
  formula=noyes~.,
  # data=train_scaled,
  data=train_scaled_reduced,
  importance=TRUE
)
train_rf_scaled

cat("\n")
cat("*** calculate error rate ***")
cat("\n")

# calculate error rate:
err_rate = function(tbl) {
  return (1-sum(diag(tbl))/sum(tbl))
}

conf_mtrx = train_rf_scaled$confusion[,1:2]
rf_err_rate = err_rate(conf_mtrx)
rf_accuracy_pct = (1-rf_err_rate)*100
print('OOB "accuracy" percent')

```

** START  

References:  
1. ISLR p.303; tree-based methods -XX  
2. ISLR p.316, 320; RF model -XX  
3. HW11, HW12 code; RF model -XX  
4. ISLR p.236; cross validation -XX  
5. HW9, HW11 and midterm code; cross validation -XX  

Steps:  
1. Create helper function for cross validation -XX  
2. Calculate across each fold -XX  
3. Plot cross validation results -XX  
4. Compare/evaluate results -XX  
5. Document observations below -XX  

Observations:  
1. Cross validation completed with 5 tries on 3 folds -XX  
2. Reduced parameters to minimize runtime -XX  
3. Plot helped visualize results -XX  
4. Model fit with ~85% accuracy and ~75% specificity -XX  
5. -XX  

END **  

```{r final-p4.2}

cat("\n")
cat("*** helper function for cross validation ***")
cat("\n")

# decrease parameters to reduce runtime:
# xvalNoYesRF = function(data, nTries=20, kXval=5) {

# decrease parameters to reduce runtime:
xvalNoYesRF = function(data, nTries=3, kXval=3) {
  retRes = NULL
  for ( iTry in 1:nTries ) {
    print(paste0('progress check, iTry: ', iTry))
    # assign each observation to one of the kXval folds
    xvalFolds = sample(rep(1:kXval, length.out=nrow(data)))
      measures <- NULL
      for ( kFold in 1:kXval ) {
        print(paste0('progress check, kFold: ', kFold))
        train = data[xvalFolds!=kFold,]
        test = data[xvalFolds==kFold,]

        # fit on the kept-folds
        # rf_fit = randomForest(formula=noyes~., data=train)
        # test_pred=predict(rf_fit, newdata=test)

        rf_fit = randomForest(train[,-1], train[,1])
        test_pred = predict(rf_fit, newdata=test[,-1])

        # convert the out-of-bag error into "accuracy" equivalent
        conf_mtrx = rf_fit$confusion[,1:2]
        rf_err_rate = err_rate(conf_mtrx)
        rf_oob_accuracy_pct = (1-rf_err_rate)*100

        # convert to numeric for sake of assess_prediction
        test_pred_num=ifelse(test_pred == 'yes', 1, 0)
        test_noyes_num = ifelse(test$noyes == 'yes', 1, 0)

        test_assessment_measures = assess_prediction(
          test_noyes_num,
          test_pred_num,
          print_results=FALSE
        )

        # accumulate test measurements over all cross-validation folds:
        measures = rbind(measures, cbind(
          test_assessment_measures$accuracy_pct,
          rf_oob_accuracy_pct,
          test_assessment_measures$sensitivity_pct,
          test_assessment_measures$specificity_pct)
        )
      }

      #print(measures)
      measure_means = colMeans(measures)
      #print(measure_means)
      retRes = rbind(retRes, data.frame(sim=iTry,
        accuracy_pct=measure_means[1],
        oob_accuracy_pct=measure_means[2],
        sensitivity_pct=measure_means[3],
        specificity_pct=measure_means[4]))
  }
  retRes
}

# decrease parameters to reduce runtime:
# number_of_folds = 5
number_of_folds = 5

# decrease parameters to reduce runtime:
# number_of_tries = 15
number_of_tries = 3

```

```{r final-p4.2A, results="hide"}

# reduce to smaller sample size to reduce runtime:
# train_scaled[1:5000,]
df_output_rf = xvalNoYesRF(
  train_scaled,
  kXval=number_of_folds,
  nTries=number_of_tries
)

```

```{r fig.width=9, fig.height=6, echo=FALSE}

cat("\n")
cat("*** cross validation plot ***")
cat("\n")

# cross validation plot:
df_output_rf_melted = melt(
  df_output_rf,
  id.vars=c('sim')
)

p = ggplot(
  df_output_rf_melted,
  aes(x=variable, y=value, colour=variable)
) + geom_boxplot()

title = sprintf(
  'Performance Measures Plot for RF; %d-fold Cross Validation',
  number_of_folds
)
p + ggtitle(title) + xlab("Measure Type") + ylab("Measure Value %")

```

# Problem 5: SVM (20 points)

Develop SVM model of categorical outcome `noyes` deciding on the choice of kernel, cost, etc. that appear to yield better performance.  Test model performance on multiple splits of data into training and test subsets, summarize model performance in terms of accuracy/error/sensitivity/specificity and compare to the performance of the rest of the models developed above (logistic regression, LDA, random forest).

** P5 **  
** P5 **  
** P5 **  

** START  

References:  
1. ISLR p.337; svm model -XX  
2. ISLR p.350; svm model -XX  
3. HW10, HW11, HW12 code; svm model -XX  
4. Lecture 10, 11 notes; svm model -XX  
5. -XX  

Steps:  
1. Prepare data to fit svm model -XX  
2. Fit svm model; also run cross validation -XX  
3. Use tune function to find best hyperparameters -XX  
4. Compare/evaluate results -XX  
5. Document observations below -XX  

Observations:  
1. Model fit with 5 attempts on 3 folds to reduce runtime -XX  
2. Linear kernal fit with ~89% accuracy -XX  
3. Polynomial kernal fit with ~88% accuracy -XX  
4. Radial kernal fit with ~90% accuracy -XX  
5. Overall, radial kernel performed best but all were close -XX  

END **  

```{r final-p5.1}

cat("\n")
cat("*** fit svm model ***")
cat("\n")

# fit svm model:
set.seed(63)
# svm_data = sample_frac(data_scaled, 0.2)
# data.frame with noyes as first column, categorical data one-hot encoded

# data_dummied = model.matrix(noyes~., train_scaled)[,-1]
data_dummied = model.matrix(noyes~., train_scaled_reduced)[,-1]

data_dummied_final = data.frame(
  # noyes=train_scaled$noyes,
  noyes=train_scaled_reduced$noyes,
  data_dummied
)

svm_data = sample_frac(
  data_dummied_final,
  0.2
)

for (kern in c('linear','polynomial', 'radial')){
  for (i in 1:3) {
    set.seed(i)
    train = sample_frac(svm_data, 0.7)
    train_idx = as.numeric(rownames(train))
    test = svm_data[-train_idx,]
    test_noyes_num = ifelse(test$noyes == 'yes', 1, 0)

    svm_fit_kern = svm(noyes~., data=train, kernel=kern)
    test_kern_predict = predict(svm_fit_kern, newdata=test)
    test_pred_kern_num=ifelse(test_kern_predict == 'yes', 1, 0)

    cat('\n\n', rep('-', 25), '\n', sep='')
    cat('pass: ', i, 'kernel: ', kern, '\n')
    results = assess_prediction(
      test_noyes_num,
      test_pred_kern_num,
      print_results=TRUE
    )
  }  
}

```

** START  

References:  
1. ISLR p.361; svm/tune/hyperparameters -XX  
2. ISLR p.372; svm/tune/hyperparameters -XX  
3. HW11 code; tune/hyperparameters -XX  
4. Lecture 10, 11 notes; svm model -XX  
5. -XX  

Steps:  
1. Prepare data to fit svm model -XX  
2. Fit svm model; also run cross validation -XX  
3. Use tune function to find best hyperparameters -XX  
4. Compare/evaluate results -XX  
5. Document observations below -XX  

Observations:  
1. Best coefficients are listed below -XX  
2. However, some data were not scaled during model fit -XX  
3. Ran out of time to investigate; hence, the error output -XX  
4. Failure to scale some data may have affected performance/output -XX  
5. Output/code are shown below for review -XX  

    degree gamma coef0 cost
 55      1  0.01     0   10

END **  

```{r final-p5.2}

cat("\n")
cat("*** use tune function to find best hyperparameters ***")
cat("\n")

# use tune function to find best hyperparameters:
svm_data = sample_frac(data_dummied_final, 0.2)

# create some ranges based around the default values for each
coef0_values = c(0, 0.01, 0.1) #1, 10, 30) # default 0
degree_values = c(1, 3, 5) #, 9, 30) # default 3
cost_values = c(0.1, 1, 10) # default 1
gamma_values=c(0.01, 0.1, 0.5) # default based on nrows 1/dim(train)[2]

tune_svm_out = tune.svm(
  x=svm_data[,-1],
  y=svm_data$noyes,
  kernel='radial',
  coef0=coef0_values,
  degree=degree_values,
  gamma=gamma_values,
  cost=cost_values
)

best_coef0 = tune_svm_out$best.parameters$coef0
best_degree = tune_svm_out$best.parameters$degree
best_cost = tune_svm_out$best.parameters$cost
best_gamma = tune_svm_out$best.parameters$gamma

print(tune_svm_out$best.parameters)

```

** START  

References:  
1. ISLR p.337; svm model -XX  
2. ISLR p.350; svm model -XX  
3. HW10, HW11, HW12 code; svm model -XX  
4. Lecture 10, 11 notes; svm model -XX  
5. -XX  

Steps:  
1. Prepare data to fit svm model -XX  
2. Fit svm model; also run cross validation -XX  
3. Use tune function to find best hyperparameters -XX  
4. Compare/evaluate results -XX  
5. Document observations below -XX  

Observations:  
1. Model fit with reduce tries/folds to reduce runtime -XX  
2. In general, model fit with ~80-85% accuracy -XX  
3. Plot helped to visualize results -XX  
4. Model fit with ~85% accruacy, ~55% sensitivity and ~95% specificity -XX  
5. Several variables did not scale as noted in the output -XX  

END **  

```{r final-p5.3}

cat("\n")
cat("*** run cross validation ***")
cat("\n")

# run cross validation:
svm_data = sample_frac(
  data_dummied_final,
  0.2
)

# decrease parameters to reduce runtime:
# xvalNoYesSvm = function(data, coef0, degree, cost, gamma, nTries=20, kXval=5) {

# decrease parameters to reduce runtime:
xvalNoYesSvm = function(data, coef0, degree, cost, gamma, nTries=5, kXval=3) {
  retRes = NULL
    for ( iTry in 1:nTries ) {
      # assign each observation to one of the kXval folds
      xvalFolds = sample(rep(1:kXval, length.out=nrow(data)))
      print(paste0('progress check, iTry: ', iTry))
      measures <- NULL

        for ( kFold in 1:kXval ) {
          print(paste0('progress check, kFold: ', kFold))
          train = data[xvalFolds!=kFold,]
          test = data[xvalFolds==kFold,]

          # fit with the "best" parameter values determined by tune.svm:
          svm_fit = svm(
            noyes~.,
            data=train,
            kernel='radial',
            coef0=best_coef0,
            degree=best_degree,
            cost=best_cost,
            gamma=best_gamma
          )

          test_predict = predict(svm_fit, newdata=test)
          test_pred_num = ifelse(test_predict == 'yes', 1, 0)
          test_noyes_num = ifelse(test$noyes== 'yes', 1, 0)

          test_assessment_measures = assess_prediction(
            test_noyes_num,
            test_pred_num,
            print_results=TRUE
          )

        # accumulate test measurements over all cross-validation folds:
        measures = rbind(measures, cbind(
          test_assessment_measures$accuracy_pct,
          test_assessment_measures$sensitivity_pct,
          test_assessment_measures$specificity_pct))
        }

          #print(measures)
          measure_means = colMeans(measures)
          #print(measure_means)
          retRes = rbind(retRes, data.frame(
            sim=iTry,
            accuracy_pct=measure_means[1],
            sensitivity_pct=measure_means[2],
            specificity_pct=measure_means[3]))

  }
  retRes
}

# decrease parameters to reduce runtime:
# number_of_folds = 15
number_of_folds = 5

# decrease parameters to reduce runtime:
# number_of_tries = 5
number_of_tries = 3

df_output_svm_tuned = xvalNoYesSvm(
  svm_data,
  coef0=best_coef0,
  degree=best_degree,
  cost=best_cost,
  gamma=best_gamma,
  nTries=number_of_tries,
  kXval=number_of_folds
)

```

```{r fig.width=9, fig.height=6, echo=FALSE}

df_svm_tuned_melted = melt(
  df_output_svm_tuned,
  id.vars=c('sim')
)

p = ggplot(
  df_svm_tuned_melted,
  aes(x=variable, y=value, colour=variable)
) + geom_boxplot()
title = sprintf(
  'Performance Measures Plot for SVM with Radial Kernel; %d-fold Cross Validation - coef0=%d, degree=%d, cost=%d, gamma=%f',
  number_of_folds,
  best_coef0,
  best_degree,
  best_cost,
  best_gamma
)
p + ggtitle(title) + xlab("Measure Type") + ylab("Measure Value %")

```

# Problem 6: predictions for test dataset  (10 points)

** P6 **  
** P6 **  
** P6 **  

## Problem 6a: compare logistic regression, LDA, random forest and SVM model performance (3 points)

Compare performance of the models developed above (logistic regression, LDA, random forest, SVM) in terms of their accuracy, error and sensitivity/specificity.  Comment on differences and similarities between them.

** P6A **  
** P6A **  
** P6A **  

** START  

References - HW9-P4.1:  
1. Midterm-P4 and HW5-P2 extra credit, cross-validation -XX  
2. Slide 33, lecture 9 notes: glm function usage -XX  
3. Slide 25, lecture 9 notes; lda function usage -XX  
4. Slide 25, lecture 9 notes; qda function usage -XX  
5. Slide 25, lecture 9 notes; knn function usage -XX  
6. Slide 25, lecture 9 notes; nb function usage -XX  

Steps:  
1. Create helper function for cross-validation -XX  
2. Function splits data into train/test -XX  
3. Make prediction with each model across dataset -XX  
4. Create box plots to visualize results -XX  
5. Compare results and document observations below -XX  

Observations:  
1. Setup prediction quality and cross validation functions for model fit -XX  
2. Cross validation had error when performing model fit -XX  
3. However, ran out of time and did not get a chance to troubleshoot -XX  
4. Code is complete and ran for several other models -XX  
5. My guess would be to troubleshoot by limiting number of variables -XX  

END **  

```{r final-p6A.1}

cat("\n")
cat("*** cross-validation calculations ***")
cat("\n")

# Reference Midterm-P4 and HW5-P2 extra credit, cross-validation:
# xval_noyes = function(data, nTries=5, kXval=5) {

# decrease parameters to reduce runtime:
xval_noyes = function(data, nTries=5, kXval=3) {
  retRes = NULL
  set.seed(65)

  for ( iTry in 1:nTries ) {
    # assign observations to kXval folds
    xvalFolds = sample(
      rep(
        1:kXval,
        length.out=nrow(data)
      )
    )
    k_models = paste0('knn', c(1,2,5,11,21,51,101))

    for ( jSelect in c('logreg', 'lda', 'qda', k_models, 'nb') ) {
      results <- NULL
      for ( kFold in 1:kXval ) {
        train = data[xvalFolds!=kFold,]
        test = data[xvalFolds==kFold,]

        # sort/match model method:
        switch (substr(jSelect, 1, 3),
          'log' = {
            # Slide 33, lecture 9 notes:
            glm_fit = glm(
              noyes~.,
              data=train,
              family=binomial
            )
            glm_predict = predict(
              glm_fit,
              newdata=test,
              type='response'
            )
            test_predict = ifelse(glm_predict > 0.5, 1, 0)
          },

          # Slide 25, lecture 9 notes:
          'lda' = {
            lda_fit = lda(
              noyes~.,
              data=train
            )
            test_predict = predict(lda_fit, newdata=test)$class
          },

          # Slide 25, lecture 9 notes:
          'qda' = {
            qda_fit = qda(
              noyes~.,
              data=train
            )
            test_predict = predict(qda_fit, newdata=test)$class
          },

          # Slide 25, lecture 9 notes:
          'knn' = {
            knn_num = as.numeric(
              substring(jSelect, 4, 6)
            )
            test_predict = knn(
              train=train,
              test=test,
              k=knn_num,
              cl=train$y_class
            )
          },

          # Slide 25, lecture 9 notes:
          'nb' = {
            nb_fit = naiveBayes(
              as.factor(noyes)~.,
              data=train
            )
            test_predict = predict(
              nb_fit,
              newdata=test,
              type='class'
            )
          }
        )

        # assess prediction quality
        predict_quality_test = assess_prediction(
          test$y_class,
          test_predict,
          print_results=FALSE
        )

        # compile results:
        results = rbind(results, cbind(
            predict_quality_test$error_rate_pct,
            predict_quality_test$sensitivity_pct,
            predict_quality_test$specificity_pct
          )
        )
      }

      print(results)
      results_avg = colMeans(results)

      retRes = rbind(retRes, data.frame(
          sim=iTry,
          model=jSelect,
          error_rate_pct=results[1],
          sensitivity_pct=results[2],
          specificity_pct=results[3]
        )
      )
    }
  }
  retRes
}

# troubleshoot null value error if time left over:
# *** ERROR ***
# Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : factor kbc has new levels fw
# *** ERROR ***

# df_output = xval_noyes(train_reduced, kXval=number_of_folds)
# df_output

```

## Problem 6b: make predictions for the **test** dataset (3 points)

Decide on the model that performs the best and use it to make predictions for the **test** dataset.  This is the dataset that is provided separately from training data without the outcome `noyes` that we are modeling here.  Upload resulting predictions in comma-separated values (CSV) format into the Canvas website.  Please check sample files with test dataset predictions for the expected format of the *.csv file with predictions: your submission must be in precisely the same format -- two and only two columns, first column - ids of the test observations ("id" column in test dataset), second - predictions as yes/no calls (not 0/1, 1/2, true/false, etc.).  The name of the second column of predictions is what will be used in leaderboard as its name.*

** P6B **  
** P6B **  
** P6B **  

** START  

References:  
1. Midterm and HW 5, 6 code; glm usage -XX  
2. ISLR p.67; p-value and significance -XX  
3. Midterm code; p-value and significance -XX  
4. ISLR p.286; logistic regression -XX  
5. ISLR p.291; logistic regression usage -XX  

Steps:  
1. Fit logistic regression model for all predictor variables -XX  
2. Evaluate/compare results of each variable -XX  
3. Specifically, evaluate p-values of logistic regression output -XX  
4. Evaluate results for association between predictor/outcome -XX  
5. Predictions included in attached CSV file -XX  

Observations:  
1. I am selecting logistic regression as my "best model" for prediction since it performs reasonably well (~80% accuracy) from my initial work and handles categorial/continuous variables. -XX  
2. I am still working on the remaining models and do not wish to risk being late on this assignment so am making the safe choice to use this model and complete my work on time. -XX  
3. Logistic regression is a safe choice and likely to outperform the 50% coin flip, hence my choice for best model. -XX  
4. One of the other models may have better performance by other metrics; however, I only have 20-30 hours to devote to this exam outside of this work this week and not 40-50 hours to perfect all nuances prior to submission. -XX  

END **  

```{r final-p6B.1}

cat("\n")
cat("*** predict with glm model ***")
cat("\n")

# *** significant factors based on p-values (1.6, logistic regression) ***
# categorial = cle, kbc, gb, xxp, rao, at, fq
# numerical = mt, zq, zf, ihj

# use the variables selected by both glm and lasso
glm_fit = glm(
  noyes~cle+kbc+xxp+rao+at+fq,
  data=train_data,
  family=binomial
)

# predict on the held-out fold
glm_predict = predict(
  glm_fit,
  newdata=test_data,
  type='response'
)

# verify output
# head(test_reduced$id, 5)
# head(glm_predict, 5)

# write output to file
write.csv(test_data$id, file = "glm_test_id.csv", row.names = FALSE)
write.csv(glm_predict, file = "glm_prediction.csv", row.names = FALSE)

```

## Problem 6c: get better than coin flip by 10% (4 points)

This is not really a problem *per se* but rather a criterion that we will go by when assessing quality of your predictions for the test dataset.  You get these four points if your predictions for **test** dataset are better than those obtained from a fair coin flip (already shown in leaderboard and as examples of the file format for predictions upload) by at least 10% on **all** four metrics shown in the leaderboard (accuracy, sensitivity, specificity and precision).  But then predictions by the coin flip should not be very difficult to improve upon.  

** P6C **  
** P6C **  
** P6C **  

** START  

References:  
1. Midterm and HW 5, 6 code; glm usage -XX  
2. ISLR p.67; p-value and significance -XX  
3. Midterm code; p-value and significance -XX  
4. ISLR p.286; logistic regression -XX  
5. ISLR p.291; logistic regression usage -XX  

Steps:  
1. Fit logistic regression model for all predictor variables -XX  
2. Evaluate/compare results of each variable -XX  
3. Specifically, evaluate p-values of logistic regression output -XX  
4. Evaluate results for association between predictor/outcome -XX  
5. Predictions included in attached CSV file -XX  

Observations:  
1. I am selecting logistic regression as my "best model" for prediction since it performs reasonably well (~80% accuracy) from my initial work and handles categorial/continuous variables. -XX  
2. I am still working on the remaining models and do not wish to risk being late on this assignment so am making the safe choice to use this model and complete my work on time. -XX  
3. Logistic regression is a safe choice and likely to outperform the 50% coin flip, hence my choice for best model. -XX  
4. One of the other models may have better performance by other metrics; however, I only have 20-30 hours to devote to this exam outside of this work this week and not 40-50 hours to perfect all nuances prior to submission. -XX  

END **  

```{r final-p6C.1}

cat("\n")
cat("*** see attached CSV file ***")
cat("\n")

# see attached CSV file

```

# Extra 10 points: neural network model

Experiment with fitting neural network models of categorical outcome `noyes` for this data and evaluate their performance on different splits of the data into training and test. Compare model performance to that for the rest of classifiers developed above.

** P7 **  
** P7 **  
** P7 **  

** START  

References - HW12-P2.1; NN classifier:  
1. HW4, HW10 and HW12-preface code; simulated dataset -XX  
2. Slide 24, 26 and 30; lecture 12 notes; neural net model -XX  
3. Lecture 12 notes; NN model -XX  
4. -XX  
5. -XX  

Steps:  
1. Create simulated dataset with 6 notes, n=10k per instructions -XX  
2. Fit neural net model; split into train/test datasets -XX  
3. Plot train/test error rates to evaluate difference -XX  
4. Compare/evaluate results -XX  
5. Document observations below -XX  

Observations:  
1. Setup code for model fit; however, error occurred during fit -XX  
2. However, ran out of time and did not get a chance to troubleshoot -XX  
3. Code is complete and ran for several other models -XX  
4. My guess would be to troubleshoot by limiting number of variables -XX  

END **  

```{r final-p7.1}
# *** DEBUG LATER ***

# *** significant factors based on p-values (1.6, logistic regression) ***
# categorial = cle, kbc, gb, xxp, rao, at, fq
# numerical = mt, zq, zf, ihj

cat("\n")
cat("*** create nn classifier ***")
cat("\n")

# create nn classifier with parameters:
```
for (i in 1:node_count) {
  nn = neuralnet(
    noyes~.mt,
    train_reduced,
    hidden=i,
    linear.output=FALSE,
    err.fct='ce',
    threshold=0.4*i
  )

  # calculate training error
  train_err = sum(
    nn$err.fct(
      nn$net.result[[1]][,1],
      nn$response
    )
  )

  nn_predict = compute(
    nn,
    train_reduced[,-train_reduced$noyes]
  )

  # calculate test error
  test_err = sum(
    nn$err.fct(
      nn_predict$net.result,
      train_reduced$noyes
    )
  )

  results = rbind(
    results,
    data.frame(
      num_nodes=i,
      train_err=train_err,
      test_err=test_err
    )
  )
}

results

** START  

References - HW12-P2.2; NN classifier:  
1. HW4, HW10 and HW12-preface code; simulated dataset -XX  
2. Slide 24, 26 and 30; lecture 12 notes; neural net model -XX  
3. HW9 code; ggplot usage -XX  
4. Lecture 12 notes; NN model -XX  
5. -XX  

Steps:  
1. Create simulated dataset per instructions -XX  
2. Fit neural net model; split into train/test datasets -XX  
3. Plot train/test error rates to evaluate difference -XX  
4. Compare/evaluate results -XX  
5. Document observations below -XX  

Observations:  
1. Setup code for model fit; however, error occurred during fit -XX  
2. However, ran out of time and did not get a chance to troubleshoot -XX  
3. Code is complete and ran for several other models -XX  
4. My guess would be to troubleshoot by limiting number of variables -XX  

END **  

```{r final-p7.2}

cat("\n")
cat("*** output here ***")
cat("\n")

# more output here:

```

```{r fig.width=8, fig.height=6, echo=FALSE}
# *** DEBUG LATER ***

# create plot below:
```
plot_melt = melt(
  results,
  id.vars='num_nodes'
)

ggplot(
  plot_melt, aes(
    x=num_nodes,
    y=value,
    colour=variable
  )) + geom_path() + geom_point() + theme(legend.position='top') + labs(x='Number of Nodes', y='Error Rate',colour='Error Type'
)


** START  

References - HW12-P4.1; NN classifier:  

References:  
1. HW4, HW10 and HW12-preface code; simulated dataset -XX  
2. Slide 24, 26 and 30; lecture 12 notes; neural net model -XX  
3. HW9, HW11 code; train/test error calculations -XX  
4. Lecture 12 notes; NN model -XX  

Steps:  
1. Fit neural network model to 10 hidden nodes -XX  
2. Plot error rates in box plots -XX  
3. Visualize hidden layers with dplyr -XX  
4. Fit with knn and svm model; calculate erorr -XX  
5. Compare error rates between model fit -XX  
6. Compare/evaluate results -XX  
7. Document observations below -XX  

Observations:  
1. Setup code for model fit; however, error occurred during fit -XX  
2. However, ran out of time and did not get a chance to troubleshoot -XX  
3. Code is complete and ran for several other models -XX  
4. My guess would be to troubleshoot by limiting number of variables -XX  

END **  

```{r final-p7.3}
# *** DEBUG LATER ***
```

cat("\n")
cat("*** create nn classifier for banknote dataset ***")
cat("\n")

cls <- as.factor(train_scaled_numeric$noyes)
set.seed(1234)

cat("\n")
cat("*** fit neural net model ***")
cat("\n")

dfTmp <- NULL
for (i in 1:10){
  for (j in 1:10){
    bTrain <- sample(
      c(FALSE,TRUE),
      nrow(train_scaled_numeric),
      replace=TRUE
    )
    # train <- dbaDat[bTrain,]
    # test <- dbaDat[!bTrain,]

    train <- train_scaled_numeric[bTrain,]
    test <- train_scaled_numeric[!bTrain,]

    # fit neural net model:
    nn <- neuralnet(
      noyes~.,
      err.fct="sse",
      data=train,
      hidden=rep(4,i)
    )
    cm <- as.matrix(table(
      test$noyes,
      1+(compute(nn, test[,1:4])$net.result>1.5))
    )
    dfTmp <- rbind(dfTmp, data.frame(
      i,err= 1-(sum(diag(cm))/sum(cm)))
    )
  }
}

cat("\n")
cat("*** create plot ***")
cat("\n")

boxplot(
  err~i,
  data=dfTmp,
  xlab="Hidden Layers"
)

library(dplyr)
dfTmp %>% group_by(i) %>% summarize(mean=mean(err),sd=sd(err))

# detact interaction between dplyr and neuralnet
detach(
  "package:dplyr",
  unload=TRUE
)

set.seed(1234)
bTrain <- sample(
  c(FALSE,TRUE),
  # nrow(dbaDat),
  nrow(train_scaled_numeric),
  replace=TRUE
)
# train <- dbaDat[bTrain,]
# test <- dbaDat[!bTrain,]

train <- train_scaled_numeric[bTrain,]
test <- train_scaled_numeric[!bTrain,]

cat("\n")
cat("*** calculate error rates ***")
cat("\n")

testError <- function(truth, predicted){
  cm <- as.matrix(
    table(
      truth,
      predicted
    )
  )
  1-(sum(diag(cm))/sum(cm))
}


** START  

References - HW12-P4.2; NN classifier:  
1. HW4, HW10 and HW12-preface code; simulated dataset -XX  
2. Slide 24, 26 and 30; lecture 12 notes; neural net model -XX  
3. HW9, HW11 code; train/test error calculations -XX  
4. Lecture 12 notes; NN model -XX  

Steps:  
1. Fit neural network model to 10 hidden nodes -XX  
2. Plot error rates in box plots -XX  
3. Visualize hidden layers with dplyr -XX  
4. Fit with knn and svm model; calculate erorr -XX  
5. Compare error rates between model fit -XX  
6. Compare/evaluate results -XX  
7. Document observations below -XX  

Observations:  
1. NN model fit on final attempt -XX  
2. Results are listed below -XX  
3. Node diagram shown to illustrate results -XX  
4. -XX  
5. -XX  

END **  

```{r final-p7.4}

# *** significant factors based on p-values (1.6, logistic regression) ***
# categorial = cle, kbc, gb, xxp, rao, at, fq
# numerical = mt, zq, zf, ihj

cat("\n")
cat("*** create plot below ***")
cat("\n")

# fit neural net model:
nm <- neuralnet(
  as.numeric(factor(noyes))~mt,
  err.fct="sse",
  data=train_reduced,
  hidden=rep(4,2)
)

cat("\n")
cat("*** create plot below ***")
cat("\n")

# create plot below:
plot(nm)

```

cat("\n")
cat("*** fit model below ***")
cat("\n")

nm <- neuralnet(
  as.numeric(factor(noyes))~.,
  err.fct="sse",
  data=train,hidden=rep(4,2)
)

cat("\n")
cat("*** calculate error below ***")
cat("\n")

nn.err <- testError(
  test$noyes,
  1+(compute(nm, test[,1:4])$net.result>1.5)
)

cat("\n")
cat("*** create plot below ***")
cat("\n")

plot(nm)

cat("\n")
cat("*** fit model below ***")
cat("\n")

km <- knn(
  train,
  test,
  k=2,
  cl=cls[bTrain]
)

cat("\n")
cat("*** calculate error below ***")
cat("\n")

kn.err <- testError(
  test$noyes,
  km
)

cat("\n")
cat("*** fit model below ***")
cat("\n")

svm <- svm(
  as.factor(noyes)~.,
  data=train,
  kernel="polynomial",
  gamma=.2,
  cost =20
)

cat("\n")
cat("*** calculate error below ***")
cat("\n")

svm.err <- testError(test$noyes,predict(
  svm,
  test[,-5])
)

# Reference HW11 code: Evaluate best performing KNN and SVM models
library(knitr)
kable(data.frame(
  nn.err,
  kn.err,
  svm.err))


```{r final-p6B.2}

cat("\n")
cat("*** predict with glm model; full test dataset output ***")
cat("\n")

# *** significant factors based on p-values (1.6, logistic regression) ***
# categorial = cle, kbc, gb, xxp, rao, at, fq
# numerical = mt, zq, zf, ihj

# use the variables selected by both glm and lasso
# glm_fit = glm(
#   noyes~cle+kbc+xxp+rao+at+fq,
#   data=train_data,
#   family=binomial
# )

# predict on the held-out fold
# glm_predict = predict(
#   glm_fit,
#   newdata=test_data,
#   type='response'
# )

# verify output; full test dataset output
# test_reduced$id
# glm_predict

```
