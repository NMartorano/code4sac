---
title: 'Code4Sac: 311 Service Call Data Analysis'
output:
  html_document:
    toc: true
---

Author: Walter Yu  
Organization: Code for Sacramento  

# Introduction

Code for Sacramento is develop a neighborhood portal application; as a result, this notebook evaluates the City of Sacramento 311 service call dataset for insights and trends which may be helpful in developing useful features. This notebook will focus on an initial analysis and modeling effort with additional prediction to follow as more information is available regarding the application.  

Dataset:

311 service call dataset from City of Sacramento; summary statistics for the full and partial datasets are listed below. The data is available in geospatial, tabular or API format; the tabular format is used for this analysis to identify important neighborhoods and trends which are relevant to developing the application.  

City of Sacramento 311 Service Call Dataset: https://data.cityofsacramento.org/datasets/08794a6695b3483f889e9bef122517e9_0  

Citations:  
1. HES CSCI E-63c: https://www.extension.harvard.edu/course-catalog/courses/elements-of-data-science-and-statistical-learning-with-r/15123  
2. ISLR Textbook: https://www-bcf.usc.edu/~gareth/ISL/  

Notes:  
1. All sources are cited accordingly.  
2. Source dataset is ~1.2M rows, so not included in this repository.  
3. As a result, please review attached HTML file for analysis/findings.  

To-Do List:
1. Spatial analysis  
2. Time series analysis  

** P0 **  
** P0 **  
** P0 **  

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Install any necessary packages:
install.packages("ggcorrplot")

library(dplyr)
library(readr)
library(stringr)
library(ggplot2)
library("ggcorrplot")
library(gridExtra)
library(leaps)
library(glmnet)
library(reshape2)
library(randomForest)
library(e1071)
library(FNN)

# LDA/QDA Packages:
library(boot)
library(MASS)
library(class)
library(reshape2)

# Neural Net Packages:
library(neuralnet)
library(tidyr)

```

** P1 **  
** P1 **  
** P1 **  

# Part 1A - Summary Statistics  

References:  
1. HES CSCI-E63c Midterm, HW9, HW10, HW11 code  
2. ISLR p.28; categorical variables  

Steps:  
1. Import data, evaluate dimensions with dim() and str() functions  
2. Subset first 5k rows to reduce runtime    
3. Subset columns to only include relevant data (time, location, type)  
4. Verify subset with summary statistics; dim, str and summary functions  

Observations:  
1. Dataset consists of categorical variables and time/location data  
2. Most categorical have multiple levels; some have many (e.g. category level)  
3. Time data recorded as timestamps; locations in lat/long coordinates  

Data Manipulation:  
1. Original dataset = ~1.2M rows x 27 columns  
2. Reduced dataset = 5k rows x 27 columns  
3. Test dataset may be extracted from full dataset  
4. Convert categorical variables as factors to facilitate logistic regression  
5. Factors will be converted to numeric values to facilitate analysis

```{r p1.1}

# import train_data:
train_data <- read.table(
  '311_Calls_OSC_View.csv',
  sep=',',
  header=TRUE,
  quote='',
  strip.white=TRUE,
  na.strings='?'
)

```

Data Attributes:  

$ X                    : num  
$ Y                    : num  
$ OBJECTID             : int  
$ ReferenceNumber      : Factor w/ 1130759 levels  
$ CategoryHierarchy    : Factor w/ 437 levels  
$ CategoryLevel1       : Factor w/ 18 levels  
$ CategoryLevel2       : Factor w/ 48 levels  
$ CategoryLevel3       : Factor w/ 10 levels  
$ CategoryLevel4       : logi  
$ CategoryLevel5       : logi  
$ CategoryName         : Factor w/ 286 levels  
$ CouncilDistrictNumber: int  
$ SourceLevel1         : Factor w/ 5 levels  
$ Neighborhood         : Factor w/ 130 levels  
$ DateCreated          : Factor w/ 1118207 levels  
$ DateUpdated          : Factor w/ 927449 levels  
$ DateClosed           : Factor w/ 772674 levels  
$ StatusType           : Factor w/ 14 levels  
$ SystemId             : int  
$ ServiceLevelName     : Factor w/ 34 levels  
$ Latitude             : num  
$ Longitude            : num  
$ XCoord               : num  
$ YCoord               : num  
$ CrossStreet          : Factor w/ 11685 levels  
$ GlobalID             : Factor w/ 1225340 levels  
$ ZIP                  : Factor w/ 66 levels  

```{r p1.2}

# verify dim and drop NA; train_data:
dim(train_data)

# reduce to smaller sample size to reduce runtime:
train_reduced = train_data[1:5000,]

# reduce to smaller sample size for relevant data:
train_col = train_reduced[, c(
  # "X",
  # "Y",
  "CategoryHierarchy",
  "CategoryName",
  "CategoryLevel1",
  "CategoryLevel2",
  "CategoryLevel3",
  "CouncilDistrictNumber",
  "SourceLevel1",
  "Neighborhood",
  "DateCreated",
  "DateUpdated",
  "DateClosed",
  "StatusType",
  "ServiceLevelName",
  "Latitude",
  "Longitude",
  "XCoord",
  "YCoord",
  "ZIP"
)]

cat("\n")
cat("*** remove na null values ***")
cat("\n")

# verify dim and drop NA; train_data:
# which(is.na(train_col))
# dim(train_col)
# train_col = na.omit(train_col)
# dim(train_col)

cat("\n")
cat("*** output summary; train_data ***")
cat("\n")

# EDA; train_data:
dim(train_col)
str(train_col)
summary(train_col)

cat("\n")
cat("*** encode categorical variables as factors ***")
cat("\n")

# Encode categorial variables as factors:
# https://stats.idre.ucla.edu/r/modules/coding-for-categorical-variables-in-regression-models/

# Selected columns for relevant data:
# "CategoryHierarchy",
# "CategoryName",
# "CategoryLevel1",
# "CategoryLevel2",
# "CategoryLevel3",
# "CouncilDistrictNumber",
# "SourceLevel1",
# "Neighborhood",
# "StatusType",
# "ServiceLevelName",
# "ZIP"

train_col$cat_heir.f <- factor(train_col$CategoryHierarchy)
is.factor(train_col$cat_heir.f)

train_col$cat_name.f <- factor(train_col$CategoryName)
is.factor(train_col$cat_name.f)

train_col$cat_1.f <- factor(train_col$CategoryLevel1)
is.factor(train_col$cat_1.f)

train_col$cat_2.f <- factor(train_col$CategoryLevel2)
is.factor(train_col$cat_2.f)

train_col$cat_3.f <- factor(train_col$CategoryLevel3)
is.factor(train_col$cat_3.f)

train_col$district.f <- factor(train_col$CouncilDistrictNumber)
is.factor(train_col$district.f)

train_col$source.f <- factor(train_col$SourceLevel1)
is.factor(train_col$source.f)

train_col$neighborhood.f <- factor(train_col$Neighborhood)
is.factor(train_col$neighborhood.f)

train_col$status.f <- factor(train_col$StatusType)
is.factor(train_col$status.f)

train_col$service.f <- factor(train_col$ServiceLevelName)
is.factor(train_col$service.f)

train_col$zip.f <- factor(train_col$ZIP)
is.factor(train_col$zip.f)

# convert into numeric values:
# train_col_numeric = train_col
# train_col_numeric %>% mutate_if(is.factor, as.numeric)

# convert with lapply function:
# https://stackoverflow.com/questions/47922184/convert-categorical-variables-to-numeric-in-r

train_col_numeric = train_col
i <- sapply(train_col_numeric, is.factor)
train_col_numeric[i] <- lapply(train_col_numeric[i], as.numeric)

# verify columns as numeric:
# str(train_col_numeric)
# head(train_col_numeric, 5)

cat("\n")
cat("*** summary statistics for numeric values ***")
cat("\n")

summary(train_col_numeric)

# save code in case prediction/validation is necessary:
# import test_data:
# test_data <- read.table(
#   './final-data-test.csv',
#   sep=',',
#   header=TRUE,
#   quote='',
#   strip.white=TRUE,
#   na.strings='?'
# )

# verify dim and drop NA; test_data:
# dim(test_data)
# na.omit(test_data)
# dim(test_data)
# head(test_data, 5)

# summary statistics; test_data:
# str(test_data)
# summary(test_data)

# reduce dataset to minimize runtime:
# test_reduced = test_data[1:1500,]

```

References:  
1. HES CSCI-E63c HW2 code  
2. https://en.wikipedia.org/wiki/Pearson_correlation_coefficient  
3. https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient  

Steps:  
1. Calculate correlation and create correlation plot   
2. Plots for Pearson and Spearman correlation methods  
3. Document observations below  

Observations:  
1. Pearson coefficient (ranges between 0 and 1) is a measure of LINEAR correlation; plots showed relatively low correlation between most variables  
2. Spearman coefficient (measured between 0 and 1) indicates MONOTONIC (linear or not) correlation; plots showed relative low correlation between most variables  
3. Although entire variables may have low correlation, there may be individual trends between variables which will be explored in the following sections  

```{r p1.2A, fig.height = 8, fig.width = 8, fig.align = "center"}

# create correlation plot:
corr <- round(cor(train_col_numeric, method="pearson", use="everything"), 4)
ggcorrplot(corr, title="Correlation Plot - Pearson (Linear)")

```

```{r p1.2B, fig.height = 8, fig.width = 8, fig.align = "center"}

# create correlation plot:
corr <- round(cor(train_col_numeric, method="spearman", use="everything"), 4)
ggcorrplot(corr, title="Correlation Plot - Spearman (Non-Linear/Monotonic)")

```

References:  
1. HES CSCI-E63c Midterm, HW5 code  
2. ISLR p.67; p-value  

Steps:  
1. Create histogram to evaluate distribution of frequency/district   
2. Create stacked bar histograms for service level and district  
3. Evaluate trends from plots; identify possible trends  
4. Create plots for significant factors based on p-values  
5. Document observations below  

Observations:  
1. Categorical variables provide limited use for histogram plots since they are intended for continuous/numeric variables  
2. However, call frequency by district number histogram showed distribution while the other plots/variables did not convert well or translate into coherent plots  

```{r p1.3}

# Create historgram for continuous variables to evaluate distribution:

hist(
  train_col_numeric$district.f,
  main='Council District',
  col='purple',
  xlab='Continuous Variable - Council District',
  breaks=30
)

hist(
  train_col_numeric$service.f,
  main='311 Service Level',
  col='purple',
  xlab='Continuous Variable - 311 Service Level',
  breaks=10
)

# Create histogram, stacked bar chart; remaining variables
qplot_district = (qplot(
  train_col_numeric$district.f,
  binwidth = 1,
  fill=train_col_numeric$service.f,
  main='Outcome Variable - 311 Service Level')
  + scale_fill_manual(values=c('blue', 'red'))
  + theme(legend.position='top')
  + xlab('Predictor Variable - Council District Number')
)
qplot_district

```

References:  
1. HES CSCI-E63c Midterm, HW9, HW10, HW11 code  
2. ISLR p.67; data distribution  
3. ISLR p.76; data distribution    

Steps:  
1. Evaluate data distribution with contingency tables  
2. Evaluate distribution, then adjust for proportions  
3. Create contingency tables for select attributes   
4. Compare/evaluate results  
5. Document observations below   

Observations:  
1. Create contingency tables for significant factors based on p-values  
2. Contingency tables show distributions of variables and breaks them down by level  
3. In general, contingency tables are good for visualizing distributions by level  

```{r p1.4}

cat("\n")
cat("*** contingency table for neighborhood and source ***")
cat("\n")

# Create contingency table:
table(
  train_data$Neighborhood,
  train_data$CategoryLevel1
)

cat("\n")
cat("*** contingency table for zipcode and source ***")
cat("\n")

# Create contingency table:
table(
  train_data$Neighborhood,
  train_data$CategoryLevel2
)

```

References:  
1. HES CSCI-E63c Lecture 1, 3 notes; chi-squared test  
2. ISLR p.67; p-value and significance  

Steps:  
1. Calculate chi-squared test for categorical variables  
2. Evaluate/analyze results  
3. Document observations below  

Observations:  
1. Chi-squared test shows low p-values for categorial variables  
2. T-test for continous variables  
3. Chi-squared and t-test are a good method for evaluating significance  

Observations:  
1. Create plots for significant factors based on p-values  

```{r p1.5}

cat("\n")
cat("*** t-test for continous variables ***")
cat("\n")

# Chi-Squared test for continuous variables:
t.test(
  as.numeric(train_col$neighborhood.f),
  train_data$service.f
)

cat("\n")
cat("*** chi-squared test for categorical variables ***")
cat("\n")

# Chi-Squared test for categorical variables:
chisq.test(
  table(
    train_col$neighborhood.f,
    train_col$cat_1.f
  )
)

```

References:  
1. HES CSCI-E63c Midterm, HW6; PCA model  
2. ISLR p.230; PCA components  
3. ISLR p.233; PCA components  

Steps:  
1. Verify/prepare data for PCA plots; use dummy variables to setup factors  
2. Scale/fit data for PCA plots; plot for top contributors to variance  
3. Evaluate top contributors to variance  
4. Plot variance by first several PCA  

Observations:  
1. Use dummy variables to address muiple variable levels prior to PCA plot  
2. PCA calculations show top contributors of variance to the model  
3. PCA plot show interaction between neighborhood and service level  

```{r p1.6}

cat("\n")
cat("*** Plot variance for first several PCA ***")
cat("\n")

# train_scaled = train_data %>% mutate_if(is.numeric, scale)
# train_dummy = model.matrix(noyes ~., train_scaled)[,-1]

# Scale/dummy data prior to plot:
# train_numeric = as.numeric(train_col$zip.f)
# train_scaled = train_numeric %>% mutate_if(is.numeric, scale)
# train_dummy = model.matrix(noyes ~., train_scaled)[,-1]

# Scale/dummy data prior to plot:
train_scaled = train_col_numeric %>% mutate_if(is.numeric, scale)
train_dummy = model.matrix(Neighborhood ~., train_scaled)[,-1]

# prepare PCA components for plot:
prcomp_plot = prcomp(train_dummy)
plot(
  prcomp_plot,
  xlab='dimensions',
  col='purple'
)

cat("\n")
cat("*** Evaluate largest loading for first PC ***")
cat("\n")

sort(abs(prcomp_plot$rotation[,1]), decreasing=TRUE)[1]

cat("\n")
cat("*** Evaluate largest loading for second PC ***")
cat("\n")

sort(abs(prcomp_plot$rotation[,2]), decreasing=TRUE)[1]

cat("\n")
cat("*** Evaluate largest loading for third PC ***")
cat("\n")

sort(abs(prcomp_plot$rotation[,3]), decreasing=TRUE)[1]

cat("\n")
cat("*** Evaluate largest loading for fourth PC ***")
cat("\n")

sort(abs(prcomp_plot$rotation[,4]), decreasing=TRUE)[1]

cat("\n")
cat("*** Evaluate largest loading for fifth PC ***")
cat("\n")

sort(abs(prcomp_plot$rotation[,5]), decreasing=TRUE)[1]

cat("\n")
cat("*** Biplot: PC1 and PC2 (Scaled) ***")
cat("\n")

# create biplot of PCA components:
biplot(
  prcomp_plot,
  pc.biplot=TRUE,
  xlabs=rep('*', dim(prcomp_plot$x)[1]),
  col=c('red','purple')
)

cat("\n")
cat("*** PCA Plot ***")
cat("\n")

# create PCA plot:
plot(
  prcomp_plot$x[,1:2],
  main='PCA Plot: Neighborhood and Service Level',
  col=c(ifelse(train_col_numeric$Neighborhood== '>=10', 'red', 'purple')),
  cex=1.0,
  pch=ifelse(train_col_numeric$ServiceLevelName== '>=1', '*', '+')
)

```

References:  
1. ISLR p.67; p-value and significance  
2. ISLR p.286; logistic regression  
3. ISLR p.291; logistic regression usage  

Steps:  
1. Fit logistic regression model for predictor variables  
2. Evaluate/compare results of each variable  
3. Specifically, evaluate p-values of logistic regression output  
4. Evaluate results for association between predictor/outcome  

Observations:  
1. Most variables have low p-values, indicating significance as a predictor  
2. Use LR to fit categorical variables for additional analysis  

```{r p1.7}

cat("\n")
cat("*** LR: cat_1 ~ neighborhood ***")
cat("\n")

# logistic regression model fit:
glm_train = glm(
  CategoryLevel1~neighborhood.f,
  data=train_col,
  family=binomial
)
summary(glm_train)

cat("\n")
cat("*** LR: cat_1 ~ district ***")
cat("\n")

# logistic regression model fit:
glm_train = glm(
  CategoryLevel1~district.f,
  data=train_col,
  family=binomial
)
summary(glm_train)

cat("\n")
cat("*** LR: cat_1 ~ zipcode ***")
cat("\n")

# logistic regression model fit:
glm_train = glm(
  CategoryLevel1~zip.f,
  data=train_col,
  family=binomial
)
summary(glm_train)

cat("\n")
cat("*** LR: cat_2 ~ neighborhood ***")
cat("\n")

# logistic regression model fit:
glm_train = glm(
  CategoryLevel2~neighborhood.f,
  data=train_col,
  family=binomial
)
summary(glm_train)

cat("\n")
cat("*** LR: cat_2 ~ district ***")
cat("\n")

# logistic regression model fit:
glm_train = glm(
  CategoryLevel2~district.f,
  data=train_col,
  family=binomial
)
summary(glm_train)

cat("\n")
cat("*** LR: cat_2 ~ zipcode ***")
cat("\n")

# logistic regression model fit:
glm_train = glm(
  CategoryLevel2~zip.f,
  data=train_col,
  family=binomial
)
summary(glm_train)

cat("\n")
cat("*** LR: cat_3 ~ neighborhood ***")
cat("\n")

# logistic regression model fit:
glm_train = glm(
  CategoryLevel3~neighborhood.f,
  data=train_col,
  family=binomial
)
summary(glm_train)

cat("\n")
cat("*** LR: cat_3 ~ district ***")
cat("\n")

# logistic regression model fit:
glm_train = glm(
  CategoryLevel3~district.f,
  data=train_col,
  family=binomial
)
summary(glm_train)

cat("\n")
cat("*** LR: cat_3 ~ zipcode ***")
cat("\n")

# logistic regression model fit:
glm_train = glm(
  CategoryLevel3~zip.f,
  data=train_col,
  family=binomial
)
summary(glm_train)

cat("\n")
cat("*** LR: neighborhood ~ cat_1 ***")
cat("\n")

# logistic regression model fit:
glm_train = glm(
  Neighborhood~cat_1.f,
  data=train_col,
  family=binomial
)
summary(glm_train)

cat("\n")
cat("*** LR: neighborhood ~ cat_2 ***")
cat("\n")

# logistic regression model fit:
glm_train = glm(
  Neighborhood~cat_2.f,
  data=train_col,
  family=binomial
)
summary(glm_train)

cat("\n")
cat("*** LR: neighborhood ~ cat_3 ***")
cat("\n")

# logistic regression model fit:
glm_train = glm(
  Neighborhood~cat_3.f,
  data=train_col,
  family=binomial
)
summary(glm_train)

cat("\n")
cat("*** LR: neighborhood ~ service level ***")
cat("\n")

# logistic regression model fit:
glm_train = glm(
  Neighborhood~service.f,
  data=train_col,
  family=binomial
)
summary(glm_train)

cat("\n")
cat("*** LR: neighborhood ~ source ***")
cat("\n")

# logistic regression model fit:
glm_train = glm(
  Neighborhood~source.f,
  data=train_col,
  family=binomial
)
summary(glm_train)

cat("\n")
cat("*** LR: zipcode ~ cat_1 ***")
cat("\n")

# logistic regression model fit:
glm_train = glm(
  ZIP~cat_1.f,
  data=train_col,
  family=binomial
)
summary(glm_train)

cat("\n")
cat("*** LR: zipcode ~ cat_2 ***")
cat("\n")

# logistic regression model fit:
glm_train = glm(
  ZIP~cat_2.f,
  data=train_col,
  family=binomial
)
summary(glm_train)

cat("\n")
cat("*** LR: zipcode ~ cat_3 ***")
cat("\n")

# logistic regression model fit:
glm_train = glm(
  ZIP~cat_3.f,
  data=train_col,
  family=binomial
)
summary(glm_train)

cat("\n")
cat("*** LR: zipcode ~ service level ***")
cat("\n")

# logistic regression model fit:
glm_train = glm(
  ZIP~service.f,
  data=train_col,
  family=binomial
)
summary(glm_train)

cat("\n")
cat("*** LR: zipcode ~ source ***")
cat("\n")

# logistic regression model fit:
glm_train = glm(
  ZIP~source.f,
  data=train_col,
  family=binomial
)
summary(glm_train)

```

# Part 2 - Logistic Regression

References:  
1. HES CSCI-E63c HW2, HW9, HW10  
1. HW2 code; pairs plot  
2. HW9 code; pairs plot  
3. HW10 code; pairs plot  

Steps:  
1. Create pairs plot for scaled, numeric predictor variables  
2. Verify that columns are numeric and use scaled dataset  
3. Create plots; document observations below  
4. Use pairs plot to verify variables for additional analysis  

Observations:  
1. Scale values so that pairs plot can be correct associations  
2. Only numeric values selected for pairs plot  
3. Run pairs plot first before moving on with remaining analysis

```{r p2.1}

# calculated here for analysis; comment out to reduce runtime
# create pairs plot of scaled numeric predictors:
# columns_is_numeric = unlist(lapply(train_reduced, is.numeric))
# numeric_cols = names(train_data)[columns_is_numeric]

# train_scaled_numeric = train_scaled %>% select_if(is.numeric)
# pairs(
#   train_scaled_numeric,
#   pch=25,
#   cex=0.8,
#   col=c(ifelse(train_scaled$source.f == '>1', 'purple', 'red')),
#   main='Pairs Plot - Source Level '
# )

```

References:  
1. HES CSCI-E63c Midterm, HW5  
2. Midterm-P2 and HW5; best subset variable selection  
3. ISLR p.244; best subset selection  
4. ISLR p.247; best subset selection  

Steps:  
1. Use variable selection to identify best predictors  
2. Use forward, backward and seq replacement methods  
3. Create metrics plot and individual plot for each method  
4. Evaluate/compare plot results  
5. Document observations below  

Observations:  
1. Each method returns the same variables and order of selection  
3. Results confirm previous findings/analysis/conjecture  
4. Significant variables will be useful for fitting other models  
5. Best subset plot visualizes results for evaluation  

```{r p2.2}

cat("\n")
cat("*** helper function for best subset selection ***")
cat("\n")

# helper function for best subset selection:
summaryMetrics <- NULL
whichAll <- list()
my_methods = c('backward', 'forward', 'seqrep')

for ( myMthd in my_methods ) {
  method_metrics = NULL
  rsRes <- regsubsets(
    Neighborhood~.,
    train_scaled,
    method=myMthd,
    nvmax=ncol(train_scaled)-1
  )
  summRes <- summary(rsRes)
  whichAll[[myMthd]] <- summRes$which

  for ( metricName in c('rsq','rss','adjr2','cp','bic') ) {
    summaryMetrics <- rbind(summaryMetrics,
      data.frame(method=myMthd,metric=metricName,
      nvars=1:length(summRes[[metricName]]),
      value=summRes[[metricName]]))
    method_metrics = rbind(method_metrics,
      data.frame(method=myMthd,metric=metricName,
      nvars=1:length(summRes[[metricName]]),
      value=summRes[[metricName]]))
  }
}

cat("\n")
cat("*** plot subset ***")
cat("\n")

# plot best subset:
ggplot(
    summaryMetrics,
    aes(x=nvars,y=value,shape=method,colour=method)
) + geom_path() + geom_point() + facet_wrap(~metric,scales='free') + theme(legend.position='top')

# plot best subset:
old.par <- par(mfrow=c(1,1),ps=9,mar=c(5,7,2,1))
for ( myMthd in names(whichAll) ) {
  image(1:nrow(whichAll[[myMthd]]),
        1:ncol(whichAll[[myMthd]]),
        whichAll[[myMthd]],xlab="N(vars)",ylab="",
        xaxt="n",yaxt="n",breaks=c(-0.5,0.5,1.5),
        col=c("white","purple"),main=myMthd)
  axis(1,1:nrow(whichAll[[myMthd]]),rownames(whichAll[[myMthd]]))
  axis(2,1:ncol(whichAll[[myMthd]]),colnames(whichAll[[myMthd]]),las=2)
}

```

References:  
1. HES CSCI-E63c Midterm, HW5  
2. ISLR p.251; lasso/ridge regression  
3. HW5, midterm code; lasso regression  
4. HW5, midterm code; ridge regression  

Steps:  
1. Fit and plot lasso model  
2. Determine best lambda values  
3. Determine lambda value; 1 SD away  
4. Document observations below  

Observations:  
1. Make calculations using scaled data  
2. Additional calculations made to evaluate error rates  
3. Error rate also calculated for 1SD away  

```{r p2.3}

cat("\n")
cat("*** fit and plot lasso model ***")
cat("\n")

# fit and plot lasso model:
# x = model.matrix(noyes~., train_scaled_reduced)[,-1]
# y = train_scaled_reduced[,'noyes']

x = model.matrix(Neighborhood~., train_scaled)[,-1]
y = train_scaled[1:4973 ,'Neighborhood']

# lasso_fit_lr = glmnet(
#   x, y,
#   alpha=1,
#   family='binomial'
# )
# plot(lasso_fit_lr)

cat("\n")
cat("*** determine best lambda values ***")
cat("\n")

# determine best lambda values:
# cv_lasso_fit_lr = cv.glmnet(
#   x, y,
#   family='binomial',
#   type.measure='class'
# )
# plot(cv_lasso_fit_lr)

cat("\n")
cat("*** determine lambda value; 1 SD away ***")
cat("\n")

# determine best lambda values; 1 SD away:
# lowest_error_plus_one_sd_lambda = cv_lasso_fit_lr$lambda.1se

# evaluate which dashed lines is best:
# print(log(lowest_error_plus_one_sd_lambda))

cat("\n")
cat("*** lasso model prediction ***")
cat("\n")

# determine best lambda values; 1 SD away:
# predict(
#   lasso_fit_lr,
#   type='coefficients',
#   s=lowest_error_plus_one_sd_lambda
# )

```

References:  
1. HES CSCI-E63c Midterm, HW5, HW6; glm model  
2. HW9-P1 code; helper function to assess prediction quality  
3. HW9-P4 code; helper function for cross validation  
4. ISLR p.236; cross validation  

Steps:  
1. Create helper function to assess prediction quality  
2. Create helper function for cross validation  
3. Print and evaluate assessment results  
4. Plot and evaluate cross validation results  
5. Document results below  

Observations:  
1. Fit based on variables selected in previous sections  
2. Measures will be used to compare against other model fits  
3. Measures plot are a good method of evaluating metrics  

```{r p2.4}

cat("\n")
cat("*** helper function to predict model quality ***")
cat("\n")

# helper function to predict model quality:
assess_prediction = function(truth, predicted, print_results=FALSE) {
  # check for missing values (we are going to
  # compute metrics on non-missing values only)
  predicted = predicted[ ! is.na(truth) ]
  truth = truth[ ! is.na(truth) ]
  truth = truth[ ! is.na(predicted) ]
  predicted = predicted[ ! is.na(predicted) ]

  NotNa=length(truth)
  # how predictions align against known
  # training/testing outcomes:
  # TP/FP= true/false positives,
  # TN/FN=true/false negatives
  TP = sum(truth==1 & predicted==1)
  TN = sum(truth==0 & predicted==0)
  FP = sum(truth==0 & predicted==1)
  FN = sum(truth==1 & predicted==0)
  P = TP+FN # total number of positives in the truth data
  N = FP+TN # total number of negatives

  accuracy_pct = signif(sum(truth==predicted)*100/length(truth),3)
  error_rate_pct = 100-accuracy_pct
  sensitivity_pct = signif(100*TP/P,3)
  specificity_pct = signif(100*TN/N,3)
  precision_pct = signif(100*TP/(TP+FP),3)
  false_discovery_pct = signif(100*FP/(TP+FP),3)
  false_positive_rate_pct = signif(100*FP/N,3)

  if (print_results){
    cat("Total cases that are not NA: ",
    NotNa,"\n",sep="")

    # overall accuracy of the test: how many cases
    # (both positive and negative) we got right:
    cat("Correct predictions (accuracy): ",
      sum(truth==predicted),
      "(",accuracy_pct,"%)\n",sep="")
    cat("TPR (sensitivity)=TP/P: ", sensitivity_pct, "%\n", sep="")
    cat("TNR (specificity)=TN/N: ", specificity_pct, "%\n", sep="")
    cat("PPV (precision)=TP/(TP+FP): ", precision_pct, "%\n", sep="")
    cat("FDR (false discovery)=1-PPV: ", false_discovery_pct, "%\n", sep="")
    cat("FPR =FP/N=1-TNR: ", false_positive_rate_pct, "%\n", sep="")

    print('TP  TN  FP  FN')
    print(paste0(TP, ' ', TN, ' ', FP, ' ', FN))
  }

  return (list(NotNA=length(truth),
              accuracy_pct=accuracy_pct,
              error_rate_pct=error_rate_pct,
              sensitivity_pct=sensitivity_pct,
              specificity_pct=specificity_pct,
              precision_pct=precision_pct,
              false_discovery_pct=false_discovery_pct,
              false_positive_rate_pct=false_positive_rate_pct))
}

cat("\n")
cat("*** helper function for cross validation ***")
cat("\n")

# helper function for cross validation:
# xvalNoYes= function(data, nTries=20, kXval=5) {

# decrease parameters to reduce runtime:
xvalLR = function(data, nTries=5, kXval=3) {
  retRes = NULL
  # set.seed(63)
  for ( iTry in 1:nTries ) {
    # assign each observation to one of the kXval folds
    xvalFolds = sample(rep(1:kXval, length.out=nrow(data)))

      measures <- NULL
      for ( kFold in 1:kXval ) {
        train = data[xvalFolds!=kFold,]
        test = data[xvalFolds==kFold,]

        # *** significant factors based on p-values (1.6, logistic regression) ***
        # categorial = cle, kbc, gb, xxp, rao, at, fq
        # numerical = mt, zq, zf, ihj

        # use the variables selected by both glm and lasso
        glm_fit = glm(
          # noyes~cle+kbc+xxp+rao+at+fq,
          CategoryLevel1~neighborhood.f,
          data=train,
          family=binomial
        )

        # predict on the held-out fold
        glm_predict = predict(glm_fit, newdata=test, type='response')

        test_predict = ifelse(glm_predict > 0.5 , 1, 0)
        test_numeric = ifelse(test$CategoryLevel1 >= 0.5, 1, 0)

        test_assessment_measures = assess_prediction(
          test_numeric,
          test_predict,
          print_results=FALSE
        )

        # accumulate test measurements over all cross-validation folds:
        measures = rbind(measures, cbind(test_assessment_measures$accuracy_pct,
          test_assessment_measures$sensitivity_pct,
          test_assessment_measures$specificity_pct))
      }

      print(measures)
      measure_means = colMeans(measures)

      print(measure_means)
      retRes = rbind(retRes, data.frame(sim=iTry,
        accuracy_pct=measure_means[1],
        sensitivity_pct=measure_means[2],
        specificity_pct=measure_means[3])
      )

  }
  retRes
}

# decrease parameters to reduce runtime:
# number_of_folds = 5
number_of_folds = 5

# decrease parameters to reduce runtime:
# number_of_tries = 15
number_of_tries = 3

```

```{r final-p2.4A, results="hide"}

# reduce to smaller sample size to reduce runtime:
# train_scaled[1:5000,]
# df_output = xvalLR(
#   train_scaled,
#   kXval=number_of_folds,
#   nTries=number_of_tries
# )

```

```{r fig.width=9, fig.height=6, echo=FALSE}

cat("\n")
cat("*** cross validation plot ***")
cat("\n")

# cross validation plot:
# df_output_melted = melt(df_output, id.vars=c('sim'))
# p = ggplot(
#   df_output_melted,
#   aes(x=variable, y=value, colour=variable)
# ) + geom_boxplot()
# title = sprintf(
#   'Performance Measures Plot for LR; %d-fold Cross Validation',
#   number_of_folds
# )
# p + ggtitle(title) + xlab("Measure Type") + ylab("Measure Value %")

```

# Conclusion

Summarize findings once analysis has been completed.
